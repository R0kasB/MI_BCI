{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of stuff was generated by gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from mne.decoding import CSP\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Import PyRiemann for Riemannian Geometry-based methods\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from pyriemann.classification import MDM\n",
    "from pyriemann.estimation import Shrinkage\n",
    "\n",
    "# Custom Transformers\n",
    "class ReshapeTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Reshapes 3D EEG data to 2D for classifier input.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        return X.reshape(n_samples, -1)\n",
    "\n",
    "class FilterTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Applies bandpass filter to EEG data.\"\"\"\n",
    "    def __init__(self, l_freq=1., h_freq=40., sfreq=256):\n",
    "        self.l_freq = l_freq\n",
    "        self.h_freq = h_freq\n",
    "        self.sfreq = sfreq\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_filtered = mne.filter.filter_data(X, sfreq=self.sfreq, l_freq=self.l_freq, h_freq=self.h_freq)\n",
    "        return X_filtered\n",
    "\n",
    "class CSPTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Applies Common Spatial Patterns (CSP) to EEG data.\"\"\"\n",
    "    def __init__(self, n_components=4, reg=None):\n",
    "        self.n_components = n_components\n",
    "        self.reg = reg\n",
    "        self.csp = CSP(n_components=self.n_components, reg=self.reg, log=True, norm_trace=False)\n",
    "    def fit(self, X, y):\n",
    "        self.csp.fit(X, y)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return self.csp.transform(X)\n",
    "\n",
    "class PSDTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extracts Power Spectral Density (PSD) features.\"\"\"\n",
    "    def __init__(self, sfreq=256, fmin=0.1, fmax=40):\n",
    "        self.sfreq = sfreq\n",
    "        self.fmin = fmin\n",
    "        self.fmax = fmax\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        psd, freqs = mne.time_frequency.psd_array_multitaper(X, sfreq=self.sfreq, fmin=self.fmin, fmax=self.fmax)\n",
    "        return psd.reshape(psd.shape[0], -1)\n",
    "\n",
    "class PCATransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Applies PCA to reduce dimensionality.\"\"\"\n",
    "    def __init__(self, n_components=10):\n",
    "        self.pca = PCA(n_components=n_components)\n",
    "    def fit(self, X, y=None):\n",
    "        n_samples = X.shape[0]\n",
    "        X_reshaped = X.reshape(n_samples, -1)\n",
    "        self.pca.fit(X_reshaped)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        X_reshaped = X.reshape(n_samples, -1)\n",
    "        return self.pca.transform(X_reshaped)\n",
    "\n",
    "class LogVarianceTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Computes the log-variance of each channel.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        # X is of shape (n_samples, n_channels, n_times)\n",
    "        log_var = np.log(np.var(X, axis=2))\n",
    "        return log_var\n",
    "\n",
    "# Define Pipelines\n",
    "pipelines = {}\n",
    "\n",
    "# Existing Pipelines with Necessary Adjustments\n",
    "\n",
    "# 1. Raw Data + SVM (Reshaping needed)\n",
    "pipelines['Raw_SVM'] = Pipeline([\n",
    "    ('reshape', ReshapeTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "# 2. Raw Data + Random Forest (Reshaping needed)\n",
    "pipelines['Raw_RF'] = Pipeline([\n",
    "    ('reshape', ReshapeTransformer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# 3. Filtered Data + SVM (Reshaping needed)\n",
    "pipelines['Filtered_SVM'] = Pipeline([\n",
    "    ('filter', FilterTransformer(l_freq=1., h_freq=40.)),\n",
    "    ('reshape', ReshapeTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "# 4. Filtered Data + Random Forest (Reshaping needed)\n",
    "pipelines['Filtered_RF'] = Pipeline([\n",
    "    ('filter', FilterTransformer(l_freq=1., h_freq=40.)),\n",
    "    ('reshape', ReshapeTransformer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# 5. CSP + LDA (No reshaping needed)\n",
    "pipelines['CSP_LDA'] = Pipeline([\n",
    "    ('filter', FilterTransformer(l_freq=8., h_freq=30.)),\n",
    "    ('csp', CSPTransformer(n_components=4)),\n",
    "    ('lda', LDA())\n",
    "])\n",
    "\n",
    "# 6. CSP + SVM (No reshaping needed)\n",
    "pipelines['CSP_SVM'] = Pipeline([\n",
    "    ('filter', FilterTransformer(l_freq=8., h_freq=30.)),\n",
    "    ('csp', CSPTransformer(n_components=4)),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "# 7. PSD Features + KNN (No reshaping needed)\n",
    "pipelines['PSD_KNN'] = Pipeline([\n",
    "    ('psd', PSDTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# 8. PSD Features + Logistic Regression (No reshaping needed)\n",
    "pipelines['PSD_LogReg'] = Pipeline([\n",
    "    ('psd', PSDTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression())\n",
    "])\n",
    "\n",
    "# 9. PCA + SVM (Reshaping is handled inside PCATransformer)\n",
    "pipelines['PCA_SVM'] = Pipeline([\n",
    "    ('pca', PCATransformer(n_components=20)),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "# 10. PCA + Random Forest (Reshaping is handled inside PCATransformer)\n",
    "pipelines['PCA_RF'] = Pipeline([\n",
    "    ('pca', PCATransformer(n_components=20)),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# 11. Bandpass Filter + CSP + SVM (No reshaping needed)\n",
    "pipelines['Bandpass_CSP_SVM'] = Pipeline([\n",
    "    ('filter', FilterTransformer(l_freq=8., h_freq=12.)),\n",
    "    ('csp', CSPTransformer(n_components=4)),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "# 12. Bandpass Filter + CSP + Random Forest (No reshaping needed)\n",
    "pipelines['Bandpass_CSP_RF'] = Pipeline([\n",
    "    ('filter', FilterTransformer(l_freq=8., h_freq=12.)),\n",
    "    ('csp', CSPTransformer(n_components=4)),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# 13. Filtered Data + LDA (Reshaping needed)\n",
    "pipelines['Filtered_LDA'] = Pipeline([\n",
    "    ('filter', FilterTransformer(l_freq=1., h_freq=40.)),\n",
    "    ('reshape', ReshapeTransformer()),\n",
    "    ('lda', LDA())\n",
    "])\n",
    "\n",
    "# 14. Raw Data + Logistic Regression (Reshaping needed)\n",
    "pipelines['Raw_LogReg'] = Pipeline([\n",
    "    ('reshape', ReshapeTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression())\n",
    "])\n",
    "\n",
    "# 15. PSD Features + SVM (No reshaping needed)\n",
    "pipelines['PSD_SVM'] = Pipeline([\n",
    "    ('psd', PSDTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "# 16. PSD Features + Random Forest (No reshaping needed)\n",
    "pipelines['PSD_RF'] = Pipeline([\n",
    "    ('psd', PSDTransformer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# 17. CSP + KNN (No reshaping needed)\n",
    "pipelines['CSP_KNN'] = Pipeline([\n",
    "    ('filter', FilterTransformer(l_freq=8., h_freq=30.)),\n",
    "    ('csp', CSPTransformer(n_components=4)),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# 18. PCA + LDA (Reshaping is handled inside PCATransformer)\n",
    "pipelines['PCA_LDA'] = Pipeline([\n",
    "    ('pca', PCATransformer(n_components=20)),\n",
    "    ('lda', LDA())\n",
    "])\n",
    "\n",
    "pipelines['Filtered_KNN'] = Pipeline([\n",
    "    ('filter', FilterTransformer(l_freq=1., h_freq=40.)),\n",
    "    ('reshape', ReshapeTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# 20. CSP + Logistic Regression (No reshaping needed)\n",
    "pipelines['CSP_LogReg'] = Pipeline([\n",
    "    ('filter', FilterTransformer(l_freq=8., h_freq=30.)),\n",
    "    ('csp', CSPTransformer(n_components=4)),\n",
    "    ('logreg', LogisticRegression())\n",
    "])\n",
    "\n",
    "# New Pipelines from MOABB Research\n",
    "\n",
    "# 21. ACM + TS + SVM (Adaptive Covariance Matrix + Tangent Space + SVM)\n",
    "pipelines['ACM_TS_SVM'] = Pipeline([\n",
    "    ('cov', Covariances(estimator='oas')),  # Adaptive covariance estimation\n",
    "    ('ts', TangentSpace()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "# 22. DLCSPauto + shLDA (CSP with automatic regularization + Shrinkage LDA)\n",
    "pipelines['DLCSPauto_shLDA'] = Pipeline([\n",
    "    ('filter', FilterTransformer(l_freq=8., h_freq=30.)),\n",
    "    ('csp', CSPTransformer(n_components=4, reg='ledoit_wolf')),\n",
    "    ('lda', LDA(solver='lsqr', shrinkage='auto'))\n",
    "])\n",
    "\n",
    "# 23. FilterBank + SVM (Filter bank CSP + SVM)\n",
    "# Note: Implementing Filter Bank CSP requires custom implementation\n",
    "# Here, we simulate it with multiple filters and concatenated CSP features\n",
    "class FilterBankCSPTransformer(BaseEstimator, TransformerMixin):\n",
    "# 19. Filtered Data + KNN (Reshaping needed)\n",
    "    \"\"\"Applies CSP over multiple frequency bands and concatenates features.\"\"\"\n",
    "    def __init__(self, frequency_bands, n_components=4):\n",
    "        self.frequency_bands = frequency_bands\n",
    "        self.n_components = n_components\n",
    "        self.csp_list = []\n",
    "    def fit(self, X, y):\n",
    "        self.csp_list = []\n",
    "        for band in self.frequency_bands:\n",
    "            csp = CSP(n_components=self.n_components, reg=None, log=True, norm_trace=False)\n",
    "            X_filtered = mne.filter.filter_data(X, sfreq=256, l_freq=band[0], h_freq=band[1])\n",
    "            csp.fit(X_filtered, y)\n",
    "            self.csp_list.append((band, csp))\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        features = []\n",
    "        for band, csp in self.csp_list:\n",
    "            X_filtered = mne.filter.filter_data(X, sfreq=256, l_freq=band[0], h_freq=band[1])\n",
    "            features.append(csp.transform(X_filtered))\n",
    "        return np.concatenate(features, axis=1)\n",
    "\n",
    "frequency_bands = [(4, 8), (8, 12), (12, 16), (16, 20), (20, 24), (24, 28), (28, 32)]\n",
    "pipelines['FilterBank_SVM'] = Pipeline([\n",
    "    ('fbcsp', FilterBankCSPTransformer(frequency_bands=frequency_bands, n_components=2)),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "# 24. FgMDM (Filter bank geometric MDM)\n",
    "# Implementing FgMDM requires complex geometric computations\n",
    "# Here we simulate it with multiple bands and MDM\n",
    "class FilterBankMDM(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Applies MDM over multiple frequency bands and averages distances.\"\"\"\n",
    "    def __init__(self, frequency_bands):\n",
    "        self.frequency_bands = frequency_bands\n",
    "        self.mdms = []\n",
    "    def fit(self, X, y):\n",
    "        self.mdms = []\n",
    "        for band in self.frequency_bands:\n",
    "            mdm = MDM()\n",
    "            X_filtered = mne.filter.filter_data(X, sfreq=256, l_freq=band[0], h_freq=band[1])\n",
    "            cov = Covariances().transform(X_filtered)\n",
    "            mdm.fit(cov, y)\n",
    "            self.mdms.append((band, mdm))\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        distances = []\n",
    "        for band, mdm in self.mdms:\n",
    "            X_filtered = mne.filter.filter_data(X, sfreq=256, l_freq=band[0], h_freq=band[1])\n",
    "            cov = Covariances().transform(X_filtered)\n",
    "            dist = mdm._predict_distances(cov)\n",
    "            distances.append(dist)\n",
    "        # Average distances over bands\n",
    "        avg_distances = np.mean(distances, axis=0)\n",
    "        return avg_distances\n",
    "\n",
    "pipelines['FgMDM'] = Pipeline([\n",
    "    ('fbgmdm', FilterBankMDM(frequency_bands=frequency_bands))\n",
    "    # Note: MDM classifier is integrated within the transformer\n",
    "])\n",
    "\n",
    "# 25. LogVariance + LDA (No reshaping needed)\n",
    "pipelines['LogVar_LDA'] = Pipeline([\n",
    "    ('logvar', LogVarianceTransformer()),\n",
    "    ('lda', LDA())\n",
    "])\n",
    "\n",
    "# 26. LogVariance + SVM (No reshaping needed)\n",
    "pipelines['LogVar_SVM'] = Pipeline([\n",
    "    ('logvar', LogVarianceTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "# 27. MDM (Covariance estimation + MDM classifier)\n",
    "pipelines['MDM'] = Pipeline([\n",
    "    ('cov', Covariances()),\n",
    "    ('mdm', MDM())\n",
    "])\n",
    "# 28. TRCSP + LDA (Temporally Regularized CSP + LDA)\n",
    "# Implementing TRCSP is complex; we can simulate with regular CSP\n",
    "pipelines['TRCSP_LDA'] = Pipeline([\n",
    "    ('filter', FilterTransformer(l_freq=8., h_freq=30.)),\n",
    "    ('csp', CSPTransformer(n_components=4, reg='oas')),  # Use regularization\n",
    "    ('lda', LDA())\n",
    "])\n",
    "\n",
    "# 29. TS + EL (Tangent Space + Ensemble Learning)\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "pipelines['TS_EL'] = Pipeline([\n",
    "    ('cov', Covariances()),\n",
    "    ('ts', TangentSpace()),\n",
    "    ('ensemble', VotingClassifier([\n",
    "        ('lda', LDA()),\n",
    "        ('svm', SVC(probability=True)),\n",
    "        ('rf', RandomForestClassifier())\n",
    "    ], voting='soft'))\n",
    "])\n",
    "\n",
    "# 30. TS + LR (Tangent Space + Logistic Regression)\n",
    "pipelines['TS_LR'] = Pipeline([\n",
    "    ('cov', Covariances()),\n",
    "    ('ts', TangentSpace()),\n",
    "    ('logreg', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# 31. TS + SVM (Tangent Space + SVM)\n",
    "pipelines['TS_SVM'] = Pipeline([\n",
    "    ('cov', Covariances()),\n",
    "    ('ts', TangentSpace()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "# Training Function with K-Fold Cross-Validation\n",
    "def train_and_evaluate(X, y, pipelines, n_splits=5):\n",
    "    \"\"\"\n",
    "    Trains and evaluates multiple pipelines using k-fold cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - X: EEG data array of shape (n_samples, n_channels, n_times)\n",
    "    - y: Labels array of shape (n_samples,)\n",
    "    - pipelines: Dictionary of sklearn Pipelines\n",
    "    - n_splits: Number of folds for cross-validation\n",
    "\n",
    "    Returns:\n",
    "    - results: Dictionary containing cross-validation scores for each pipeline\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    results = {}\n",
    "\n",
    "    for name, pipeline in pipelines.items():\n",
    "        print(f\"Evaluating pipeline: {name}\")\n",
    "        try:\n",
    "            scores = cross_val_score(pipeline, X, y, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "            print(f\"Scores: {scores}\")\n",
    "            print(f\"Mean accuracy: {np.mean(scores):.4f}\")\n",
    "            results[name] = scores\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred in pipeline {name}: {e}\")\n",
    "            results[name] = None\n",
    "    return results\n",
    "\n",
    "# Example Usage\n",
    "# Assuming `raw` is your MNE Raw object and `labels` is your labels array\n",
    "# Preprocess raw data to get numpy array\n",
    "def preprocess_raw(raw, event_id, tmin, tmax):\n",
    "    \"\"\"\n",
    "    Extracts epochs and returns data and labels.\n",
    "\n",
    "    Parameters:\n",
    "    - raw: MNE Raw object\n",
    "    - event_id: Dictionary mapping event names to event IDs\n",
    "    - tmin: Start time before event\n",
    "    - tmax: End time after event\n",
    "\n",
    "    Returns:\n",
    "    - X: EEG data array of shape (n_epochs, n_channels, n_times)\n",
    "    - y: Labels array of shape (n_epochs,)\n",
    "    \"\"\"\n",
    "    events = mne.find_events(raw)\n",
    "    picks = mne.pick_types(raw.info, meg=False, eeg=True, eog=False, stim=False)\n",
    "    epochs = mne.Epochs(raw, events, event_id, tmin, tmax, proj=True, picks=picks, baseline=None, preload=True)\n",
    "    X = epochs.get_data()  # shape: (n_epochs, n_channels, n_times)\n",
    "    y = epochs.events[:, -1]\n",
    "    return X, y\n",
    "\n",
    "# Example call\n",
    "# X, y = preprocess_raw(raw, event_id={'Stimulus': 1}, tmin=0, tmax=1)\n",
    "# results = train_and_evaluate(X, y, pipelines, n_splits=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from mne.decoding import CSP, Vectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "\n",
    "# Custom Transformers\n",
    "class ReshapeTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Reshapes 3D EEG data to 2D for classifier input.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        return X.reshape(n_samples, -1)\n",
    "\n",
    "class HilbertTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Applies the Hilbert transform to EEG data.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        # Apply Hilbert transform along the time axis\n",
    "        analytic_signal = mne.filter.hilbert(X, picks=None, envelope=False)\n",
    "        amplitude_envelope = np.abs(analytic_signal)\n",
    "        return amplitude_envelope\n",
    "\n",
    "class TimeDomainTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Computes time-domain features like mean, variance, skewness.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        # Compute features along the time axis\n",
    "        mean = np.mean(X, axis=2)\n",
    "        var = np.var(X, axis=2)\n",
    "        skewness = np.mean(((X - mean[:, :, np.newaxis]) ** 3), axis=2) / (var ** 1.5)\n",
    "        features = np.concatenate((mean, var, skewness), axis=1)\n",
    "        return features\n",
    "\n",
    "class MeanAmplitudeTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Computes the mean amplitude of the signal.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        mean_amplitude = np.mean(np.abs(X), axis=2)\n",
    "        return mean_amplitude\n",
    "\n",
    "class ARTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Computes Autoregressive (AR) coefficients.\"\"\"\n",
    "    def __init__(self, order=5):\n",
    "        self.order = order\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        from statsmodels.tsa.ar_model import AutoReg\n",
    "        n_samples, n_channels, n_times = X.shape\n",
    "        ar_features = np.zeros((n_samples, n_channels * self.order))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_channels):\n",
    "                model = AutoReg(X[i, j, :], lags=self.order, old_names=False)\n",
    "                model_fit = model.fit()\n",
    "                ar_coeffs = model_fit.params[1:]  # Exclude intercept\n",
    "                ar_features[i, j * self.order:(j + 1) * self.order] = ar_coeffs\n",
    "        return ar_features\n",
    "\n",
    "class STFTTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Computes Short-Time Fourier Transform (STFT).\"\"\"\n",
    "    def __init__(self, n_fft=256):\n",
    "        self.n_fft = n_fft\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        from scipy.signal import stft\n",
    "        n_samples, n_channels, n_times = X.shape\n",
    "        stft_features = []\n",
    "        for i in range(n_samples):\n",
    "            sample_features = []\n",
    "            for j in range(n_channels):\n",
    "                f, t, Zxx = stft(X[i, j, :], nperseg=self.n_fft)\n",
    "                sample_features.append(np.abs(Zxx).flatten())\n",
    "            stft_features.append(np.concatenate(sample_features))\n",
    "        return np.array(stft_features)\n",
    "\n",
    "class WaveletTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Applies Wavelet Transform.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        import pywt\n",
    "        n_samples, n_channels, n_times = X.shape\n",
    "        wavelet_features = []\n",
    "        for i in range(n_samples):\n",
    "            sample_features = []\n",
    "            for j in range(n_channels):\n",
    "                coeffs = pywt.wavedec(X[i, j, :], 'db4', level=3)\n",
    "                coeffs_flat = np.concatenate([c.flatten() for c in coeffs])\n",
    "                sample_features.append(coeffs_flat)\n",
    "            wavelet_features.append(np.concatenate(sample_features))\n",
    "        return np.array(wavelet_features)\n",
    "\n",
    "class WPDTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Applies Wavelet Packet Decomposition (WPD).\"\"\"\n",
    "import mne\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        import pywt\n",
    "        n_samples, n_channels, n_times = X.shape\n",
    "        wpd_features = []\n",
    "            sample_features = []\n",
    "        for i in range(n_samples):\n",
    "                wp = pywt.WaveletPacket(X[i, j, :], 'db4', mode='symmetric', maxlevel=3)\n",
    "            for j in range(n_channels):\n",
    "                nodes = wp.get_level(3, order='freq')\n",
    "                coeffs = np.array([n.data for n in nodes], 'd')\n",
    "            wpd_features.append(np.concatenate(sample_features))\n",
    "        return np.array(wpd_features)\n",
    "\n",
    "class FFTTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Computes Fast Fourier Transform (FFT).\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        fft_coeffs = np.fft.rfft(X, axis=2)\n",
    "        fft_features = np.abs(fft_coeffs)\n",
    "        return fft_features.reshape(X.shape[0], -1)\n",
    "\n",
    "class CARTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Applies Common Average Referencing (CAR).\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        car = X - np.mean(X, axis=1, keepdims=True)\n",
    "        return car\n",
    "\n",
    "class LaplacianTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Applies Laplacian spatial filtering.\"\"\"\n",
    "    def __init__(self, info):\n",
    "        self.info = info\n",
    "        self.laplacian = None\n",
    "    def fit(self, X, y=None):\n",
    "        from mne.channels import make_standard_montage\n",
    "        montage = make_standard_montage('standard_1020')\n",
    "        self.info.set_montage(montage)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        laplacian = mne.preprocessing.compute_current_source_density(self.info)\n",
    "        return laplacian.get_data()\n",
    "\n",
    "class ICATransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Applies Independent Component Analysis (ICA).\"\"\"\n",
    "    def __init__(self, n_components=15, random_state=42):\n",
    "        self.n_components = n_components\n",
    "        self.random_state = random_state\n",
    "        self.ica = None\n",
    "    def fit(self, X, y=None):\n",
    "        n_samples, n_channels, n_times = X.shape\n",
    "        self.ica = mne.preprocessing.ICA(n_components=self.n_components, random_state=self.random_state)\n",
    "        X_concat = np.concatenate(X, axis=1)\n",
    "        info = mne.create_info(n_channels, sfreq=256, ch_types='eeg')\n",
    "        raw = mne.io.RawArray(X_concat, info)\n",
    "        self.ica.fit(raw)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        n_samples, n_channels, n_times = X.shape\n",
    "        X_transformed = []\n",
    "        for i in range(n_samples):\n",
    "            info = mne.create_info(n_channels, sfreq=256, ch_types='eeg')\n",
    "            raw = mne.io.RawArray(X[i], info)\n",
    "            X_ica = self.ica.apply(raw.copy(), exclude=[])\n",
    "            X_transformed.append(X_ica.get_data())\n",
    "        return np.array(X_transformed)\n",
    "\n",
    "# Define Pipelines\n",
    "pipelines = {}\n",
    "\n",
    "# Existing pipelines adjusted to remove bandpass filtering\n",
    "# (assuming bandpass filtering is done separately)\n",
    "\n",
    "# 1. CSP + Logistic Regression (No reshaping needed)\n",
    "pipelines['CSP_LogReg'] = Pipeline([\n",
    "    ('csp', CSP(n_components=4, reg=None, log=True, norm_trace=False)),\n",
    "    ('logreg', LogisticRegression())\n",
    "])\n",
    "\n",
    "# 2. PSD + SVM (No reshaping needed)\n",
    "pipelines['PSD_SVM'] = Pipeline([\n",
    "    ('psd', PSDTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "# 3. Time Domain Features + Random Forest (Reshaping needed)\n",
    "pipelines['TimeDomain_RF'] = Pipeline([\n",
    "    ('time_features', TimeDomainTransformer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# 4. Hilbert Transform + k-NN (No reshaping needed)\n",
    "pipelines['Hilbert_KNN'] = Pipeline([\n",
    "    ('hilbert', HilbertTransformer()),\n",
    "    ('reshape', ReshapeTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# 5. Wavelet Transform + PSD + Naive Bayes\n",
    "pipelines['Wavelet_PSD_NB'] = Pipeline([\n",
    "    ('wavelet', WaveletTransformer()),\n",
    "    ('psd', PSDTransformer()),\n",
    "    ('nb', GaussianNB())\n",
    "])\n",
    "\n",
    "# 6. CAR + CSP + Decision Tree\n",
    "pipelines['CAR_CSP_DT'] = Pipeline([\n",
    "    ('car', CARTransformer()),\n",
    "    ('csp', CSP(n_components=4, reg=None, log=True, norm_trace=False)),\n",
    "    ('dt', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "# 7. ICA + Time Domain Features + SVM\n",
    "pipelines['ICA_TimeDomain_SVM'] = Pipeline([\n",
    "    ('ica', ICATransformer(n_components=15)),\n",
    "    ('time_features', TimeDomainTransformer()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "# 8. CSP + LDA (No reshaping needed)\n",
    "pipelines['CSP_LDA'] = Pipeline([\n",
    "    ('csp', CSP(n_components=4, reg=None, log=True, norm_trace=False)),\n",
    "    ('lda', LDA())\n",
    "])\n",
    "\n",
    "# 9. PSD + Gradient Boosting\n",
    "pipelines['PSD_GB'] = Pipeline([\n",
    "    ('psd', PSDTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "# 10. CAR + Riemannian Geometry Features + Logistic Regression\n",
    "pipelines['CAR_Riemann_LogReg'] = Pipeline([\n",
    "    ('car', CARTransformer()),\n",
    "    ('cov', Covariances()),\n",
    "    ('ts', TangentSpace()),\n",
    "    ('logreg', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# 11. STFT + SVM\n",
    "pipelines['STFT_SVM'] = Pipeline([\n",
    "    ('stft', STFTTransformer(n_fft=256)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "# 12. Laplacian + Wavelet Packet Decomposition + Random Forest\n",
    "# Note: LaplacianTransformer requires EEG info object\n",
    "# Adjust accordingly in your code\n",
    "# Assuming `info` is available\n",
    "pipelines['Laplacian_WPD_RF'] = Pipeline([\n",
    "    # ('laplacian', LaplacianTransformer(info)),  # Uncomment and set info appropriately\n",
    "    ('wpd', WPDTransformer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# 13. Hilbert Transform + CNN\n",
    "# Note: Implementing CNNs requires deep learning frameworks like TensorFlow or PyTorch\n",
    "# Sklearn does not support CNNs directly, so we'll skip this pipeline or note its complexity\n",
    "\n",
    "# 14. Time-Frequency Analysis (Morlet Wavelets) + k-NN\n",
    "class MorletWaveletTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Applies time-frequency analysis using Morlet wavelets.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        freqs = np.linspace(8, 30, num=22)  # Frequencies from 8 to 30 Hz\n",
    "        n_samples, n_channels, n_times = X.shape\n",
    "        power_features = []\n",
    "        for i in range(n_samples):\n",
    "            sample_features = []\n",
    "            for j in range(n_channels):\n",
    "                power = mne.time_frequency.tfr_array_morlet(X[i:i+1, j:j+1, :], sfreq=256, freqs=freqs, n_cycles=7, output='power')\n",
    "                sample_features.append(power.flatten())\n",
    "            power_features.append(np.concatenate(sample_features))\n",
    "        return np.array(power_features)\n",
    "\n",
    "pipelines['Morlet_KNN'] = Pipeline([\n",
    "    ('morlet', MorletWaveletTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# 15. AR Coefficients + Logistic Regression\n",
    "pipelines['AR_LogReg'] = Pipeline([\n",
    "    ('ar', ARTransformer(order=5)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# 16. RNN implementation is complex and not directly supported in scikit-learn\n",
    "# We'll skip this pipeline or note its complexity\n",
    "\n",
    "# 17. Mean Amplitude Features + Naive Bayes\n",
    "pipelines['MeanAmp_NB'] = Pipeline([\n",
    "    ('mean_amp', MeanAmplitudeTransformer()),\n",
    "    ('nb', GaussianNB())\n",
    "])\n",
    "\n",
    "# 18. ICA + PSD + SVM\n",
    "pipelines['ICA_PSD_SVM'] = Pipeline([\n",
    "    ('ica', ICATransformer(n_components=15)),\n",
    "    ('psd', PSDTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "# 19. FFT + Random Forest\n",
    "pipelines['FFT_RF'] = Pipeline([\n",
    "    ('fft', FFTTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# 20. Time-Frequency Features + LDA\n",
    "# Using STFT for time-frequency features\n",
    "pipelines['TimeFreq_LDA'] = Pipeline([\n",
    "    ('stft', STFTTransformer(n_fft=256)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lda', LDA())\n",
    "])\n",
    "\n",
    "# Training Function with K-Fold Cross-Validation\n",
    "def train_and_evaluate(X, y, pipelines, n_splits=5):\n",
    "    \"\"\"\n",
    "    Trains and evaluates multiple pipelines using k-fold cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - X: EEG data array of shape (n_samples, n_channels, n_times)\n",
    "    - y: Labels array of shape (n_samples,)\n",
    "    - pipelines: Dictionary of sklearn Pipelines\n",
    "    - n_splits: Number of folds for cross-validation\n",
    "\n",
    "    Returns:\n",
    "    - results: Dictionary containing cross-validation scores for each pipeline\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    results = {}\n",
    "\n",
    "    for name, pipeline in pipelines.items():\n",
    "        print(f\"Evaluating pipeline: {name}\")\n",
    "        try:\n",
    "            scores = cross_val_score(pipeline, X, y, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "            print(f\"Scores: {scores}\")\n",
    "            print(f\"Mean accuracy: {np.mean(scores):.4f}\")\n",
    "            results[name] = scores\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred in pipeline {name}: {e}\")\n",
    "            results[name] = None\n",
    "    return results\n",
    "\n",
    "# Example Usage\n",
    "# Assuming `X` and `y` are your EEG data and labels\n",
    "# Adjust the code to match your data loading and preprocessing steps\n",
    "\n",
    "# Example call\n",
    "# results = train_and_evaluate(X, y, pipelines, n_splits=5)\n",
    "\n",
    "                sample_features.append(coeffs.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "su NN ats   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from mne.decoding import CSP\n",
    "from sklearn.decomposition import PCA\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "\n",
    "# Import for Deep Learning and XGBoost\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, DepthwiseConv2D, SeparableConv2D, BatchNormalization, Activation, AveragePooling2D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Custom Transformers\n",
    "class ReshapeTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Reshapes 3D EEG data to 2D for classifier input.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        return X.reshape(n_samples, -1)\n",
    "\n",
    "class HilbertTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Applies the Hilbert transform to EEG data.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        # Apply Hilbert transform along the time axis\n",
    "        analytic_signal = mne.filter.hilbert(X, picks=None, envelope=False)\n",
    "        amplitude_envelope = np.abs(analytic_signal)\n",
    "        return amplitude_envelope\n",
    "\n",
    "class TimeDomainTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Computes time-domain features like mean, variance, skewness.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        # Compute features along the time axis\n",
    "        mean = np.mean(X, axis=2)\n",
    "        var = np.var(X, axis=2)\n",
    "        skewness = np.mean(((X - mean[:, :, np.newaxis]) ** 3), axis=2) / (var ** 1.5)\n",
    "        features = np.concatenate((mean, var, skewness), axis=1)\n",
    "        return features\n",
    "\n",
    "class MeanAmplitudeTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Computes the mean amplitude of the signal.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        mean_amplitude = np.mean(np.abs(X), axis=2)\n",
    "        return mean_amplitude\n",
    "\n",
    "class ARTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Computes Autoregressive (AR) coefficients.\"\"\"\n",
    "    def __init__(self, order=5):\n",
    "        self.order = order\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        from statsmodels.tsa.ar_model import AutoReg\n",
    "        n_samples, n_channels, n_times = X.shape\n",
    "        ar_features = np.zeros((n_samples, n_channels * self.order))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_channels):\n",
    "                model = AutoReg(X[i, j, :], lags=self.order, old_names=False)\n",
    "                model_fit = model.fit()\n",
    "                ar_coeffs = model_fit.params[1:]  # Exclude intercept\n",
    "                ar_features[i, j * self.order:(j + 1) * self.order] = ar_coeffs\n",
    "        return ar_features\n",
    "\n",
    "class STFTTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Computes Short-Time Fourier Transform (STFT).\"\"\"\n",
    "    def __init__(self, n_fft=256):\n",
    "        self.n_fft = n_fft\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        from scipy.signal import stft\n",
    "        n_samples, n_channels, n_times = X.shape\n",
    "        stft_features = []\n",
    "        for i in range(n_samples):\n",
    "            sample_features = []\n",
    "            for j in range(n_channels):\n",
    "                f, t, Zxx = stft(X[i, j, :], nperseg=self.n_fft)\n",
    "                sample_features.append(np.abs(Zxx).flatten())\n",
    "            stft_features.append(np.concatenate(sample_features))\n",
    "        return np.array(stft_features)\n",
    "\n",
    "class WaveletTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Applies Wavelet Transform.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        import pywt\n",
    "        n_samples, n_channels, n_times = X.shape\n",
    "        wavelet_features = []\n",
    "        for i in range(n_samples):\n",
    "            sample_features = []\n",
    "            for j in range(n_channels):\n",
    "                coeffs = pywt.wavedec(X[i, j, :], 'db4', level=3)\n",
    "                coeffs_flat = np.concatenate([c.flatten() for c in coeffs])\n",
    "                sample_features.append(coeffs_flat)\n",
    "            wavelet_features.append(np.concatenate(sample_features))\n",
    "        return np.array(wavelet_features)\n",
    "\n",
    "class WPDTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Applies Wavelet Packet Decomposition (WPD).\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        import pywt\n",
    "        n_samples, n_channels, n_times = X.shape\n",
    "        wpd_features = []\n",
    "        for i in range(n_samples):\n",
    "            sample_features = []\n",
    "            for j in range(n_channels):\n",
    "                wp = pywt.WaveletPacket(X[i, j, :], 'db4', mode='symmetric', maxlevel=3)\n",
    "                nodes = wp.get_level(3, order='freq')\n",
    "                coeffs = np.array([n.data for n in nodes], 'd')\n",
    "                sample_features.append(coeffs.flatten())\n",
    "            wpd_features.append(np.concatenate(sample_features))\n",
    "        return np.array(wpd_features)\n",
    "\n",
    "class FFTTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Computes Fast Fourier Transform (FFT).\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        fft_coeffs = np.fft.rfft(X, axis=2)\n",
    "        fft_features = np.abs(fft_coeffs)\n",
    "        return fft_features.reshape(X.shape[0], -1)\n",
    "\n",
    "class CARTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Applies Common Average Referencing (CAR).\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        car = X - np.mean(X, axis=1, keepdims=True)\n",
    "        return car\n",
    "\n",
    "class ICATransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Applies Independent Component Analysis (ICA).\"\"\"\n",
    "    def __init__(self, n_components=15, random_state=42):\n",
    "        self.n_components = n_components\n",
    "        self.random_state = random_state\n",
    "        self.ica = None\n",
    "    def fit(self, X, y=None):\n",
    "        n_samples, n_channels, n_times = X.shape\n",
    "        X_concat = X.reshape(n_samples * n_channels, n_times)\n",
    "        self.ica = mne.preprocessing.ICA(n_components=self.n_components, random_state=self.random_state, max_iter='auto')\n",
    "        info = mne.create_info(ch_names=['eeg'] * X_concat.shape[0], sfreq=256, ch_types='eeg')\n",
    "        raw = mne.io.RawArray(X_concat, info)\n",
    "        self.ica.fit(raw)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        n_samples, n_channels, n_times = X.shape\n",
    "        X_transformed = []\n",
    "        for i in range(n_samples):\n",
    "            data = X[i]\n",
    "            info = mne.create_info(ch_names=['eeg'] * n_channels, sfreq=256, ch_types='eeg')\n",
    "            raw = mne.io.RawArray(data, info)\n",
    "            raw_ica = self.ica.apply(raw.copy(), exclude=[])\n",
    "            X_transformed.append(raw_ica.get_data())\n",
    "        return np.array(X_transformed)\n",
    "\n",
    "class EEGNetTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Transformer that reshapes data for EEGNet.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        # Reshape X to (n_samples, n_channels, n_times, 1)\n",
    "        n_samples, n_channels, n_times = X.shape\n",
    "        X_reshaped = X.reshape(n_samples, n_channels, n_times, 1)\n",
    "        return X_reshaped\n",
    "\n",
    "class MorletWaveletTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Applies time-frequency analysis using Morlet wavelets.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        freqs = np.linspace(8, 30, num=22)  # Frequencies from 8 to 30 Hz\n",
    "        n_samples, n_channels, n_times = X.shape\n",
    "        power_features = []\n",
    "        for i in range(n_samples):\n",
    "            sample_features = []\n",
    "            for j in range(n_channels):\n",
    "                power = mne.time_frequency.tfr_array_morlet(X[i:i+1, j:j+1, :], sfreq=256, freqs=freqs, n_cycles=7, output='power')\n",
    "                sample_features.append(power.flatten())\n",
    "            power_features.append(np.concatenate(sample_features))\n",
    "        return np.array(power_features)\n",
    "\n",
    "# Define Pipelines\n",
    "pipelines = {}\n",
    "\n",
    "# Existing pipelines adjusted to remove bandpass filtering\n",
    "# (assuming bandpass filtering is done separately)\n",
    "\n",
    "# 1. CSP + Logistic Regression (No reshaping needed)\n",
    "pipelines['CSP_LogReg'] = Pipeline([\n",
    "    ('csp', CSP(n_components=4, reg=None, log=True, norm_trace=False)),\n",
    "    ('logreg', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# 2. PSD + SVM (No reshaping needed)\n",
    "pipelines['PSD_SVM'] = Pipeline([\n",
    "    ('psd', PSDTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "# ... [Include all previously defined pipelines here] ...\n",
    "\n",
    "# 19. EEGNet (Deep CNN)\n",
    "def create_eegnet_model(n_channels, n_times, n_classes):\n",
    "    \"\"\"Creates an EEGNet model.\"\"\"\n",
    "    input_shape = (n_channels, n_times, 1)\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Block 1\n",
    "    x = Conv2D(16, (1, 64), padding='same',\n",
    "               use_bias=False)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = DepthwiseConv2D((n_channels, 1), use_bias=False,\n",
    "                        depth_multiplier=2,\n",
    "                        depthwise_constraint=max_norm(1.))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 4))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = SeparableConv2D(32, (1, 16), use_bias=False, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 8))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dense(n_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Placeholder function to create the model with the correct input dimensions\n",
    "def create_eegnet_model_wrapper():\n",
    "    n_channels = X.shape[1]\n",
    "    n_times = X.shape[2]\n",
    "    n_classes = len(np.unique(y))\n",
    "    return create_eegnet_model(n_channels, n_times, n_classes)\n",
    "\n",
    "pipelines['EEGNet_CNN'] = Pipeline([\n",
    "    ('reshape', EEGNetTransformer()),\n",
    "    ('eegnet', KerasClassifier(build_fn=create_eegnet_model_wrapper, epochs=50, batch_size=16, verbose=0))\n",
    "])\n",
    "\n",
    "# 20. PSD Features + XGBoost Classifier\n",
    "pipelines['PSD_XGB'] = Pipeline([\n",
    "    ('psd', PSDTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('xgb', xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n",
    "])\n",
    "\n",
    "# 21. Ensemble Methods (Stacking Classifier)\n",
    "pipelines['Stacking'] = Pipeline([\n",
    "    ('psd', PSDTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('stacking', StackingClassifier(\n",
    "        estimators=[\n",
    "            ('svm', SVC(probability=True)),\n",
    "            ('rf', RandomForestClassifier()),\n",
    "            ('knn', KNeighborsClassifier())\n",
    "        ],\n",
    "        final_estimator=LogisticRegression(max_iter=1000),\n",
    "        cv=5\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 22. Sparse Representation Classification (SRC)\n",
    "# Note: Implementing SRC requires additional libraries or custom code.\n",
    "# For illustration purposes, we will simulate SRC using Lasso regression.\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "class SRCClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Simulates a Sparse Representation Classifier using Lasso.\"\"\"\n",
    "    def __init__(self, alpha=0.1):\n",
    "        self.alpha = alpha\n",
    "        self.classes_ = None\n",
    "        self.dictionary_ = None\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.dictionary_ = {}\n",
    "        for cls in self.classes_:\n",
    "            self.dictionary_[cls] = X[y == cls].T\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        preds = []\n",
    "        for x in X:\n",
    "            residuals = []\n",
    "            for cls in self.classes_:\n",
    "                lasso = Lasso(alpha=self.alpha, max_iter=1000)\n",
    "                lasso.fit(self.dictionary_[cls], x)\n",
    "                reconstruction = lasso.predict(self.dictionary_[cls])\n",
    "                residual = np.linalg.norm(x - reconstruction)\n",
    "                residuals.append(residual)\n",
    "            preds.append(self.classes_[np.argmin(residuals)])\n",
    "        return np.array(preds)\n",
    "\n",
    "pipelines['SRC'] = Pipeline([\n",
    "    ('reshape', ReshapeTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('src', SRCClassifier(alpha=0.1))\n",
    "])\n",
    "\n",
    "# 23. Multilayer Perceptron (MLP) Neural Network\n",
    "pipelines['MLP_NN'] = Pipeline([\n",
    "    ('reshape', ReshapeTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('mlp', MLPClassifier(hidden_layer_sizes=(100,), max_iter=500))\n",
    "])\n",
    "\n",
    "# Training Function with K-Fold Cross-Validation\n",
    "def train_and_evaluate(X, y, pipelines, n_splits=5):\n",
    "    \"\"\"\n",
    "    Trains and evaluates multiple pipelines using k-fold cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - X: EEG data array of shape (n_samples, n_channels, n_times)\n",
    "    - y: Labels array of shape (n_samples,)\n",
    "    - pipelines: Dictionary of sklearn Pipelines\n",
    "    - n_splits: Number of folds for cross-validation\n",
    "\n",
    "    Returns:\n",
    "    - results: Dictionary containing cross-validation scores for each pipeline\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    results = {}\n",
    "\n",
    "    for name, pipeline in pipelines.items():\n",
    "        print(f\"Evaluating pipeline: {name}\")\n",
    "        try:\n",
    "            # For EEGNet, we need to adjust the data shape inside the cross-validation\n",
    "            if name == 'EEGNet_CNN':\n",
    "                # Use StratifiedKFold to maintain class balance\n",
    "                from sklearn.model_selection import StratifiedKFold\n",
    "                skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "                scores = []\n",
    "                for train_index, test_index in skf.split(X, y):\n",
    "                    X_train, X_test = X[train_index], X[test_index]\n",
    "                    y_train, y_test = y[train_index], y[test_index]\n",
    "                    pipeline.fit(X_train, y_train)\n",
    "                    score = pipeline.score(X_test, y_test)\n",
    "                    scores.append(score)\n",
    "                scores = np.array(scores)\n",
    "            else:\n",
    "                scores = cross_val_score(pipeline, X, y, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "            print(f\"Scores: {scores}\")\n",
    "            print(f\"Mean accuracy: {np.mean(scores):.4f}\")\n",
    "            results[name] = scores\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred in pipeline {name}: {e}\")\n",
    "            results[name] = None\n",
    "    return results\n",
    "\n",
    "# Example Usage\n",
    "# Assuming `X` and `y` are your EEG data and labels\n",
    "# Adjust the code to match your data loading and preprocessing steps\n",
    "\n",
    "# Example call\n",
    "# results = train_and_evaluate(X, y, pipelines, n_splits=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import mne\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,\n",
    "                              VotingClassifier, StackingClassifier)\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "from mne.decoding import CSP, Vectorizer\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "\n",
    "# Import for Deep Learning models\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Conv2D, DepthwiseConv2D, SeparableConv2D,\n",
    "                                     BatchNormalization, Activation, AveragePooling2D,\n",
    "                                     Dropout, Flatten, Dense)\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# ===========================================\n",
    "# Custom Transformers (Use built-in where possible)\n",
    "# ===========================================\n",
    "\n",
    "# Since some transformations are not available in built-in libraries,\n",
    "# we define custom transformers only when necessary.\n",
    "\n",
    "class ReshapeTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Reshapes 3D EEG data to 2D for classifier input.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        return X.reshape(n_samples, -1)\n",
    "\n",
    "class PSDTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extracts Power Spectral Density (PSD) features.\"\"\"\n",
    "    def __init__(self, sfreq=256, fmin=0.1, fmax=40):\n",
    "        self.sfreq = sfreq\n",
    "        self.fmin = fmin\n",
    "        self.fmax = fmax\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        psd, freqs = mne.time_frequency.psd_array_multitaper(\n",
    "            X, sfreq=self.sfreq, fmin=self.fmin, fmax=self.fmax, verbose=False)\n",
    "        return psd.reshape(psd.shape[0], -1)\n",
    "\n",
    "class ARTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Computes Autoregressive (AR) coefficients.\"\"\"\n",
    "    def __init__(self, order=5):\n",
    "        self.order = order\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        from statsmodels.tsa.ar_model import AutoReg\n",
    "        n_samples, n_channels, n_times = X.shape\n",
    "        ar_features = np.zeros((n_samples, n_channels * self.order))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_channels):\n",
    "                model = AutoReg(X[i, j, :], lags=self.order, old_names=False)\n",
    "                model_fit = model.fit()\n",
    "                ar_coeffs = model_fit.params[1:]  # Exclude intercept\n",
    "                ar_features[i, j * self.order:(j + 1) * self.order] = ar_coeffs\n",
    "        return ar_features\n",
    "\n",
    "# ===========================================\n",
    "# Machine Learning Pipelines\n",
    "# ===========================================\n",
    "\n",
    "ml_pipelines = {}\n",
    "\n",
    "# 1. CSP + Logistic Regression\n",
    "ml_pipelines['CSP_LogReg'] = Pipeline([\n",
    "    ('csp', CSP(n_components=4, reg=None, log=True, norm_trace=False)),\n",
    "    ('logreg', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# 2. PSD + SVM\n",
    "ml_pipelines['PSD_SVM'] = Pipeline([\n",
    "    ('psd', PSDTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "# 3. Time Domain Features + Random Forest\n",
    "class TimeDomainTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Computes time-domain features like mean, variance, skewness.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        mean = np.mean(X, axis=2)\n",
    "        var = np.var(X, axis=2)\n",
    "        skewness = np.mean(((X - mean[:, :, np.newaxis]) ** 3), axis=2) / (var ** 1.5)\n",
    "        features = np.concatenate((mean, var, skewness), axis=1)\n",
    "        return features\n",
    "\n",
    "ml_pipelines['TimeDomain_RF'] = Pipeline([\n",
    "    ('time_features', TimeDomainTransformer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# 4. Hilbert Transform + k-NN\n",
    "class HilbertTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Applies the Hilbert transform to EEG data.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        analytic_signal = mne.filter.hilbert(X, picks=None, envelope=False, verbose=False)\n",
    "        amplitude_envelope = np.abs(analytic_signal)\n",
    "        return amplitude_envelope\n",
    "\n",
    "ml_pipelines['Hilbert_KNN'] = Pipeline([\n",
    "    ('hilbert', HilbertTransformer()),\n",
    "    ('reshape', ReshapeTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# 5. Wavelet Transform + PSD + Naive Bayes\n",
    "class WaveletTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Applies Wavelet Transform.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        import pywt\n",
    "        n_samples, n_channels, n_times = X.shape\n",
    "        wavelet_features = []\n",
    "        for i in range(n_samples):\n",
    "            sample_features = []\n",
    "            for j in range(n_channels):\n",
    "                coeffs = pywt.wavedec(X[i, j, :], 'db4', level=3)\n",
    "                coeffs_flat = np.concatenate([c.flatten() for c in coeffs])\n",
    "                sample_features.append(coeffs_flat)\n",
    "            wavelet_features.append(np.concatenate(sample_features))\n",
    "        return np.array(wavelet_features)\n",
    "\n",
    "ml_pipelines['Wavelet_PSD_NB'] = Pipeline([\n",
    "    ('wavelet', WaveletTransformer()),\n",
    "    ('psd', PSDTransformer()),\n",
    "    ('nb', GaussianNB())\n",
    "])\n",
    "\n",
    "# 6. CAR + CSP + Decision Tree\n",
    "class CARTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Applies Common Average Referencing (CAR).\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        car = X - np.mean(X, axis=1, keepdims=True)\n",
    "        return car\n",
    "\n",
    "ml_pipelines['CAR_CSP_DT'] = Pipeline([\n",
    "    ('car', CARTransformer()),\n",
    "    ('csp', CSP(n_components=4, reg=None, log=True, norm_trace=False)),\n",
    "    ('dt', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "# 7. ICA + Time Domain Features + SVM\n",
    "class ICATransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Applies Independent Component Analysis (ICA).\"\"\"\n",
    "    def __init__(self, n_components=15, random_state=42):\n",
    "        self.n_components = n_components\n",
    "        self.random_state = random_state\n",
    "        self.ica = None\n",
    "    def fit(self, X, y=None):\n",
    "        n_samples, n_channels, n_times = X.shape\n",
    "        X_concat = X.reshape(n_samples * n_channels, n_times)\n",
    "        self.ica = mne.preprocessing.ICA(n_components=self.n_components,\n",
    "                                         random_state=self.random_state,\n",
    "                                         max_iter='auto', verbose=False)\n",
    "        info = mne.create_info(ch_names=['eeg'] * X_concat.shape[0],\n",
    "                               sfreq=256, ch_types='eeg')\n",
    "        raw = mne.io.RawArray(X_concat, info, verbose=False)\n",
    "        self.ica.fit(raw)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        n_samples, n_channels, n_times = X.shape\n",
    "        X_transformed = []\n",
    "        for i in range(n_samples):\n",
    "            data = X[i]\n",
    "            info = mne.create_info(ch_names=['eeg'] * n_channels,\n",
    "                                   sfreq=256, ch_types='eeg')\n",
    "            raw = mne.io.RawArray(data, info, verbose=False)\n",
    "            raw_ica = self.ica.apply(raw.copy(), exclude=[], verbose=False)\n",
    "            X_transformed.append(raw_ica.get_data())\n",
    "        return np.array(X_transformed)\n",
    "\n",
    "ml_pipelines['ICA_TimeDomain_SVM'] = Pipeline([\n",
    "    ('ica', ICATransformer(n_components=15)),\n",
    "    ('time_features', TimeDomainTransformer()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "# 8. CSP + LDA\n",
    "ml_pipelines['CSP_LDA'] = Pipeline([\n",
    "    ('csp', CSP(n_components=4, reg=None, log=True, norm_trace=False)),\n",
    "    ('lda', LDA())\n",
    "])\n",
    "\n",
    "# 9. PSD + Gradient Boosting\n",
    "ml_pipelines['PSD_GB'] = Pipeline([\n",
    "    ('psd', PSDTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "# 10. CAR + Riemannian Geometry Features + Logistic Regression\n",
    "ml_pipelines['CAR_Riemann_LogReg'] = Pipeline([\n",
    "    ('car', CARTransformer()),\n",
    "    ('cov', Covariances()),\n",
    "    ('ts', TangentSpace()),\n",
    "    ('logreg', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# 11. STFT + SVM\n",
    "class STFTTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Computes Short-Time Fourier Transform (STFT).\"\"\"\n",
    "    def __init__(self, n_fft=256):\n",
    "        self.n_fft = n_fft\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        from scipy.signal import stft\n",
    "        n_samples, n_channels, n_times = X.shape\n",
    "        stft_features = []\n",
    "        for i in range(n_samples):\n",
    "            sample_features = []\n",
    "            for j in range(n_channels):\n",
    "                _, _, Zxx = stft(X[i, j, :], nperseg=self.n_fft)\n",
    "                sample_features.append(np.abs(Zxx).flatten())\n",
    "            stft_features.append(np.concatenate(sample_features))\n",
    "        return np.array(stft_features)\n",
    "\n",
    "ml_pipelines['STFT_SVM'] = Pipeline([\n",
    "    ('stft', STFTTransformer(n_fft=256)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "# 12. Morlet Wavelet Transform + k-NN\n",
    "class MorletWaveletTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Applies time-frequency analysis using Morlet wavelets.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        freqs = np.linspace(8, 30, num=22)\n",
    "        n_samples, n_channels, n_times = X.shape\n",
    "        power_features = []\n",
    "        for i in range(n_samples):\n",
    "            sample_features = []\n",
    "            for j in range(n_channels):\n",
    "                power = mne.time_frequency.tfr_array_morlet(\n",
    "                    X[i:i+1, j:j+1, :], sfreq=256, freqs=freqs,\n",
    "                    n_cycles=7, output='power', verbose=False)\n",
    "                sample_features.append(power.flatten())\n",
    "            power_features.append(np.concatenate(sample_features))\n",
    "        return np.array(power_features)\n",
    "\n",
    "ml_pipelines['Morlet_KNN'] = Pipeline([\n",
    "    ('morlet', MorletWaveletTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# 13. AR Coefficients + Logistic Regression\n",
    "ml_pipelines['AR_LogReg'] = Pipeline([\n",
    "    ('ar', ARTransformer(order=5)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# 14. Mean Amplitude Features + Naive Bayes\n",
    "class MeanAmplitudeTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Computes the mean amplitude of the signal.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        mean_amplitude = np.mean(np.abs(X), axis=2)\n",
    "        return mean_amplitude\n",
    "\n",
    "ml_pipelines['MeanAmp_NB'] = Pipeline([\n",
    "    ('mean_amp', MeanAmplitudeTransformer()),\n",
    "    ('nb', GaussianNB())\n",
    "])\n",
    "\n",
    "# 15. ICA + PSD + SVM\n",
    "ml_pipelines['ICA_PSD_SVM'] = Pipeline([\n",
    "    ('ica', ICATransformer(n_components=15)),\n",
    "    ('psd', PSDTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "# 16. FFT + Random Forest\n",
    "class FFTTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Computes Fast Fourier Transform (FFT).\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        fft_coeffs = np.fft.rfft(X, axis=2)\n",
    "        fft_features = np.abs(fft_coeffs)\n",
    "        return fft_features.reshape(X.shape[0], -1)\n",
    "\n",
    "ml_pipelines['FFT_RF'] = Pipeline([\n",
    "    ('fft', FFTTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# 17. Time-Frequency Features (STFT) + LDA\n",
    "ml_pipelines['TimeFreq_LDA'] = Pipeline([\n",
    "    ('stft', STFTTransformer(n_fft=256)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lda', LDA())\n",
    "])\n",
    "\n",
    "# 18. PSD Features + XGBoost Classifier\n",
    "ml_pipelines['PSD_XGB'] = Pipeline([\n",
    "    ('psd', PSDTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('xgb', xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n",
    "])\n",
    "\n",
    "# 19. Ensemble Methods (Stacking Classifier)\n",
    "ml_pipelines['Stacking'] = Pipeline([\n",
    "    ('psd', PSDTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('stacking', StackingClassifier(\n",
    "        estimators=[\n",
    "            ('svm', SVC(probability=True)),\n",
    "            ('rf', RandomForestClassifier()),\n",
    "            ('knn', KNeighborsClassifier())\n",
    "        ],\n",
    "        final_estimator=LogisticRegression(max_iter=1000),\n",
    "        cv=5\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 20. Sparse Representation Classification (SRC)\n",
    "class SRCClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Simulates a Sparse Representation Classifier using Lasso.\"\"\"\n",
    "    def __init__(self, alpha=0.1):\n",
    "        self.alpha = alpha\n",
    "        self.classes_ = None\n",
    "        self.dictionary_ = None\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.dictionary_ = {}\n",
    "        for cls in self.classes_:\n",
    "            self.dictionary_[cls] = X[y == cls].T\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        preds = []\n",
    "        for x in X:\n",
    "            residuals = []\n",
    "            for cls in self.classes_:\n",
    "                lasso = Lasso(alpha=self.alpha, max_iter=1000)\n",
    "                lasso.fit(self.dictionary_[cls], x)\n",
    "                reconstruction = lasso.predict(self.dictionary_[cls])\n",
    "                residual = np.linalg.norm(x - reconstruction)\n",
    "                residuals.append(residual)\n",
    "            preds.append(self.classes_[np.argmin(residuals)])\n",
    "        return np.array(preds)\n",
    "\n",
    "ml_pipelines['SRC'] = Pipeline([\n",
    "    ('reshape', ReshapeTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('src', SRCClassifier(alpha=0.1))\n",
    "])\n",
    "\n",
    "# 21. Multilayer Perceptron (MLP) Neural Network (As ML Pipeline)\n",
    "ml_pipelines['MLP_NN'] = Pipeline([\n",
    "    ('reshape', ReshapeTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('mlp', MLPClassifier(hidden_layer_sizes=(100,), max_iter=500))\n",
    "])\n",
    "\n",
    "# ===========================================\n",
    "# Neural Network Pipelines\n",
    "# ===========================================\n",
    "\n",
    "nn_pipelines = {}\n",
    "\n",
    "# 1. EEGNet (Deep CNN)\n",
    "class EEGNetTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Transformer that reshapes data for EEGNet.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        # Reshape X to (n_samples, n_channels, n_times, 1)\n",
    "        n_samples, n_channels, n_times = X.shape\n",
    "        X_reshaped = X.reshape(n_samples, n_channels, n_times, 1)\n",
    "        return X_reshaped\n",
    "\n",
    "def create_eegnet_model(n_channels, n_times, n_classes):\n",
    "    \"\"\"Creates an EEGNet model.\"\"\"\n",
    "    input_shape = (n_channels, n_times, 1)\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Block 1\n",
    "    x = Conv2D(16, (1, 64), padding='same',\n",
    "               use_bias=False)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = DepthwiseConv2D((n_channels, 1), use_bias=False,\n",
    "                        depth_multiplier=2,\n",
    "                        depthwise_constraint=max_norm(1.))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 4))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = SeparableConv2D(32, (1, 16), use_bias=False, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 8))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dense(n_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Placeholder function to create the model with the correct input dimensions\n",
    "def create_eegnet_model_wrapper():\n",
    "    n_channels = X.shape[1]\n",
    "    n_times = X.shape[2]\n",
    "    n_classes = len(np.unique(y))\n",
    "    return create_eegnet_model(n_channels, n_times, n_classes)\n",
    "\n",
    "nn_pipelines['EEGNet_CNN'] = Pipeline([\n",
    "    ('reshape', EEGNetTransformer()),\n",
    "    ('eegnet', KerasClassifier(build_fn=create_eegnet_model_wrapper, epochs=50, batch_size=16, verbose=0))\n",
    "])\n",
    "\n",
    "# ===========================================\n",
    "# Training Function with K-Fold Cross-Validation\n",
    "# ===========================================\n",
    "\n",
    "def train_and_evaluate(X, y, pipelines, n_splits=5):\n",
    "    \"\"\"\n",
    "    Trains and evaluates multiple pipelines using k-fold cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - X: EEG data array of shape (n_samples, n_channels, n_times)\n",
    "    - y: Labels array of shape (n_samples,)\n",
    "    - pipelines: Dictionary of sklearn Pipelines\n",
    "    - n_splits: Number of folds for cross-validation\n",
    "\n",
    "    Returns:\n",
    "    - results: Dictionary containing cross-validation scores for each pipeline\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for name, pipeline in pipelines.items():\n",
    "        print(f\"Evaluating pipeline: {name}\")\n",
    "        try:\n",
    "            if name == 'EEGNet_CNN':\n",
    "                # Use StratifiedKFold for EEGNet to maintain class balance\n",
    "                skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "                scores = []\n",
    "                for train_index, test_index in skf.split(X, y):\n",
    "                    X_train, X_test = X[train_index], X[test_index]\n",
    "                    y_train, y_test = y[train_index], y[test_index]\n",
    "                    pipeline.fit(X_train, y_train)\n",
    "                    score = pipeline.score(X_test, y_test)\n",
    "                    scores.append(score)\n",
    "                scores = np.array(scores)\n",
    "            else:\n",
    "                kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "                scores = cross_val_score(pipeline, X, y, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "            print(f\"Scores: {scores}\")\n",
    "            print(f\"Mean accuracy: {np.mean(scores):.4f}\")\n",
    "            results[name] = scores\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred in pipeline {name}: {e}\")\n",
    "            results[name] = None\n",
    "    return results\n",
    "\n",
    "# ===========================================\n",
    "# Example Usage\n",
    "# ===========================================\n",
    "\n",
    "# Assuming `X` and `y` are your EEG data and labels\n",
    "# Adjust the code to match your data loading and preprocessing steps\n",
    "\n",
    "# Example call for Machine Learning Pipelines\n",
    "# results_ml = train_and_evaluate(X, y, ml_pipelines, n_splits=5)\n",
    "\n",
    "# Example call for Neural Network Pipelines\n",
    "# results_nn = train_and_evaluate(X, y, nn_pipelines, n_splits=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of Pipelines for Mental Imagery BCI\n",
    "\n",
    "Below is a detailed explanation of each of the pipelines mentioned. For each pipeline, I'll describe:\n",
    "\n",
    "- **Preprocessing Steps**: Any data preprocessing or transformations applied before feature extraction.\n",
    "- **Feature Extraction**: Methods used to extract features from the EEG data.\n",
    "- **Classifier**: The machine learning algorithm used for classification.\n",
    "- **Notes**: Any additional information or considerations.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Common Spatial Patterns (CSP) + Logistic Regression (`CSP_LogReg`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Common Spatial Patterns (CSP)**:\n",
    "  - **Purpose**: CSP is a feature extraction method that projects multi-channel EEG data into a low-dimensional spatial subspace that maximizes the variance for one class while minimizing it for the other.\n",
    "  - **How It Works**: CSP computes spatial filters that optimize the discriminability between two classes by maximizing the variance of one class while minimizing the variance of the other.\n",
    "  - **Output**: CSP transforms the EEG data into a set of features (usually the log-variance of the filtered signals), resulting in a feature matrix suitable for classification.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Logistic Regression**:\n",
    "  - A linear classifier that models the probability of class membership using the logistic function.\n",
    "  - **Advantages**: Simple, interpretable, and performs well with linearly separable data.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **CSP Assumptions**: Works best when data is bandpass filtered to relevant frequency bands (e.g., mu and beta rhythms).\n",
    "- **Binary Classification**: CSP is typically used for binary classification tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Power Spectral Density (PSD) + Support Vector Machine (SVM) (`PSD_SVM`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Power Spectral Density (PSD)**:\n",
    "  - **Purpose**: PSD measures the power of the EEG signal at different frequency components.\n",
    "  - **How It Works**: Using methods like Welch's method or multitaper, the EEG data is transformed into the frequency domain, and the power at each frequency bin is calculated.\n",
    "  - **Output**: A feature vector representing the power distribution across frequencies for each channel.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Support Vector Machine (SVM)**:\n",
    "  - A supervised learning algorithm that finds the optimal hyperplane to separate classes.\n",
    "  - **Kernel Trick**: Can use different kernel functions (linear, polynomial, RBF) to handle non-linearly separable data.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Feature Dimensionality**: PSD features can be high-dimensional; dimensionality reduction or feature selection may be beneficial.\n",
    "- **SVM Advantages**: Effective in high-dimensional spaces and with clear margin separation.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Time Domain Features + Random Forest (`TimeDomain_RF`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Time-Domain Features**:\n",
    "  - **Mean**: Average value of the EEG signal over time.\n",
    "  - **Variance**: Measure of signal variability.\n",
    "  - **Skewness**: Measure of asymmetry in the signal distribution.\n",
    "  - **How It Works**: These statistical features are computed across the time dimension for each channel.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Random Forest**:\n",
    "  - An ensemble method using multiple decision trees.\n",
    "  - **How It Works**: Each tree is trained on a bootstrap sample with random feature selection; the final prediction is made by aggregating the outputs of all trees (e.g., majority vote).\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Non-linear Relationships**: Random forests can capture non-linear relationships between features and target classes.\n",
    "- **Feature Importance**: Random forests provide measures of feature importance, useful for feature selection.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Hilbert Transform + k-Nearest Neighbors (k-NN) (`Hilbert_KNN`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Hilbert Transform**:\n",
    "  - **Purpose**: To obtain the analytic signal from a real-valued signal, allowing extraction of instantaneous amplitude and phase.\n",
    "  - **How It Works**: The Hilbert transform is applied to the EEG signal to compute the amplitude envelope or phase information.\n",
    "  - **Output**: Amplitude envelope of the EEG signal for each channel.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **k-Nearest Neighbors (k-NN)**:\n",
    "  - A non-parametric classifier that assigns a class based on the majority class among the k nearest neighbors in the feature space.\n",
    "  - **Distance Metrics**: Commonly uses Euclidean distance, but other metrics can be applied.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Parameter Tuning**: The choice of `k` and the distance metric can significantly impact performance.\n",
    "- **Computational Cost**: k-NN can be computationally expensive for large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Wavelet Transform + PSD + Naive Bayes (`Wavelet_PSD_NB`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Wavelet Transform**:\n",
    "  - **Purpose**: Decomposes the EEG signal into time-frequency components with good time and frequency resolution.\n",
    "  - **How It Works**: Applies wavelet decomposition (e.g., using Daubechies wavelets) to capture signal characteristics at various scales.\n",
    "  - **Output**: Wavelet coefficients representing the signal at different scales and positions.\n",
    "- **Power Spectral Density (PSD)**:\n",
    "  - Calculated on the wavelet coefficients to obtain power distribution across scales.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Naive Bayes**:\n",
    "  - A probabilistic classifier based on Bayes' theorem, assuming feature independence.\n",
    "  - **Advantages**: Simple, fast, and works well with high-dimensional data.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Feature Independence Assumption**: Naive Bayes assumes features are independent, which may not hold for EEG data.\n",
    "- **Wavelet Selection**: The choice of wavelet function and decomposition level can affect feature quality.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Common Average Referencing (CAR) + CSP + Decision Tree (`CAR_CSP_DT`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Common Average Referencing (CAR)**:\n",
    "  - **Purpose**: To reduce common noise across all channels.\n",
    "  - **How It Works**: Subtracts the average signal across all channels from each channel's signal.\n",
    "  - **Output**: Referenced EEG data with reduced artifacts.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Common Spatial Patterns (CSP)**:\n",
    "  - As described in Pipeline 1.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Decision Tree**:\n",
    "  - A tree-based classifier that splits data based on feature thresholds to maximize class separation.\n",
    "  - **Advantages**: Simple to interpret, handles both numerical and categorical data.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Overfitting Risk**: Decision trees can overfit; pruning or setting maximum depth can mitigate this.\n",
    "- **CAR Benefits**: CAR can enhance signal-to-noise ratio, benefiting CSP feature extraction.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Independent Component Analysis (ICA) + Time Domain Features + SVM (`ICA_TimeDomain_SVM`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Independent Component Analysis (ICA)**:\n",
    "  - **Purpose**: To separate mixed signals into statistically independent components.\n",
    "  - **How It Works**: Decomposes EEG signals into independent sources, which can help isolate artifacts (e.g., eye blinks, muscle movements).\n",
    "  - **Output**: Cleaned EEG data or components representing neural activity.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Time-Domain Features**:\n",
    "  - As described in Pipeline 3.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Support Vector Machine (SVM)**:\n",
    "  - As described in Pipeline 2.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Artifact Removal**: ICA can effectively remove artifacts when components corresponding to noise are identified and excluded.\n",
    "- **Complexity**: ICA requires careful application to avoid removing neural signals of interest.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. CSP + Linear Discriminant Analysis (LDA) (`CSP_LDA`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Common Spatial Patterns (CSP)**:\n",
    "  - As described in Pipeline 1.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Linear Discriminant Analysis (LDA)**:\n",
    "  - A linear classifier that projects data onto a line to maximize class separation.\n",
    "  - **How It Works**: Finds a linear combination of features that best separates the classes.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Popular Combination**: CSP + LDA is a classic approach in motor imagery BCI applications due to its simplicity and effectiveness.\n",
    "- **Assumptions**: LDA assumes normally distributed features with equal covariance matrices for each class.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. PSD + Gradient Boosting Classifier (`PSD_GB`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Power Spectral Density (PSD)**:\n",
    "  - As described in Pipeline 2.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Gradient Boosting Classifier**:\n",
    "  - An ensemble method that builds sequential weak learners (typically decision trees), where each new learner focuses on correcting the errors of previous ones.\n",
    "  - **Advantages**: High performance, can model complex relationships.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Hyperparameter Tuning**: Requires careful tuning of parameters like learning rate, number of estimators, and tree depth.\n",
    "- **Overfitting Risk**: Can overfit if not properly regularized.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. CAR + Riemannian Geometry Features + Logistic Regression (`CAR_Riemann_LogReg`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Common Average Referencing (CAR)**:\n",
    "  - As described in Pipeline 6.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Riemannian Geometry Features**:\n",
    "  - **Covariance Matrices**: Compute the covariance matrix of EEG signals for each trial.\n",
    "  - **Tangent Space Mapping**: Maps covariance matrices to the tangent space of the Riemannian manifold to obtain feature vectors.\n",
    "  - **Purpose**: Captures the spatial covariance structure of EEG signals, which is informative for classification.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Logistic Regression**:\n",
    "  - As described in Pipeline 1.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Riemannian Methods**: Effective for BCI applications due to robustness to noise and ability to capture complex signal structures.\n",
    "- **Computational Cost**: Calculating covariance matrices and tangent space mapping can be computationally intensive.\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Short-Time Fourier Transform (STFT) + SVM (`STFT_SVM`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Short-Time Fourier Transform (STFT)**:\n",
    "  - **Purpose**: Analyzes the signal's frequency content over time by applying the Fourier Transform to short overlapping time windows.\n",
    "  - **How It Works**: Divides the EEG signal into segments, applies windowing, and computes the Fourier Transform for each segment.\n",
    "  - **Output**: Time-frequency representation of the signal.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Support Vector Machine (SVM)**:\n",
    "  - As described in Pipeline 2.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Feature Dimensionality**: STFT features can be very high-dimensional; dimensionality reduction may be necessary.\n",
    "- **Time-Frequency Analysis**: Useful for capturing non-stationary properties of EEG signals.\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Laplacian Spatial Filtering + Wavelet Packet Decomposition (WPD) + Random Forest (`Laplacian_WPD_RF`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Laplacian Spatial Filtering**:\n",
    "  - **Purpose**: Enhances spatial resolution by emphasizing local activity and reducing volume conduction effects.\n",
    "  - **How It Works**: Computes the difference between a channel and the average of its neighboring channels.\n",
    "  - **Output**: Spatially filtered EEG data.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Wavelet Packet Decomposition (WPD)**:\n",
    "  - **Purpose**: Decomposes the signal into a set of frequency subbands with both time and frequency localization.\n",
    "  - **How It Works**: Applies wavelet packet analysis to the EEG data, providing a more detailed frequency analysis than standard wavelet transforms.\n",
    "  - **Output**: Coefficients representing the signal's energy at various scales and positions.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Random Forest**:\n",
    "  - As described in Pipeline 3.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Information Preservation**: WPD can capture subtle signal features that may be relevant for classification.\n",
    "- **Computational Complexity**: WPD can be computationally intensive due to the extensive decomposition.\n",
    "\n",
    "---\n",
    "\n",
    "## 13. Morlet Wavelets + k-NN (`Morlet_KNN`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Time-Frequency Analysis using Morlet Wavelets**:\n",
    "  - **Purpose**: Captures time-frequency characteristics of EEG signals with high resolution.\n",
    "  - **How It Works**: Applies Morlet wavelet transform, which provides a complex exponential modulated by a Gaussian, ideal for EEG analysis.\n",
    "  - **Output**: Power spectra across frequencies and time.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **k-Nearest Neighbors (k-NN)**:\n",
    "  - As described in Pipeline 4.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Frequency Selection**: Frequencies of interest (e.g., 8-30 Hz) are analyzed.\n",
    "- **Data Dimensionality**: The resulting features may be high-dimensional, affecting computational cost.\n",
    "\n",
    "---\n",
    "\n",
    "## 14. Autoregressive (AR) Coefficients + Logistic Regression (`AR_LogReg`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Autoregressive (AR) Coefficients**:\n",
    "  - **Purpose**: Models the EEG signal as a linear function of its previous values.\n",
    "  - **How It Works**: Fits an AR model to the EEG time series, extracting the coefficients as features.\n",
    "  - **Output**: AR coefficients for each channel.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Logistic Regression**:\n",
    "  - As described in Pipeline 1.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Model Order**: The choice of AR model order (number of lags) affects feature quality.\n",
    "- **Stationarity Assumption**: AR models assume signal stationarity within the analysis window.\n",
    "\n",
    "---\n",
    "\n",
    "## 15. Mean Amplitude Features + Naive Bayes (`MeanAmp_NB`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Mean Amplitude**:\n",
    "  - **Purpose**: Simple feature representing the average absolute value of the EEG signal.\n",
    "  - **How It Works**: Computes the mean of the absolute values of the EEG signal over time for each channel.\n",
    "  - **Output**: Feature vector of mean amplitudes.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Naive Bayes**:\n",
    "  - As described in Pipeline 5.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Simplicity**: This approach uses straightforward features and a simple classifier.\n",
    "- **Performance**: May not capture complex signal characteristics, potentially limiting classification accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## 16. ICA + PSD + SVM (`ICA_PSD_SVM`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Independent Component Analysis (ICA)**:\n",
    "  - As described in Pipeline 7.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Power Spectral Density (PSD)**:\n",
    "  - Calculated on the ICA components to analyze the frequency content of the independent sources.\n",
    "  - **Output**: PSD features derived from the independent components.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Support Vector Machine (SVM)**:\n",
    "  - As described in Pipeline 2.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Artifact Removal**: ICA can help isolate neural signals from artifacts before feature extraction.\n",
    "- **Component Selection**: Choosing relevant components is crucial; including artifact components can degrade performance.\n",
    "\n",
    "---\n",
    "\n",
    "## 17. Fast Fourier Transform (FFT) + Random Forest (`FFT_RF`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Fast Fourier Transform (FFT)**:\n",
    "  - **Purpose**: Transforms the time-domain EEG signal into the frequency domain.\n",
    "  - **How It Works**: Computes the discrete Fourier Transform efficiently to obtain frequency coefficients.\n",
    "  - **Output**: Magnitude of the FFT coefficients for each channel.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Random Forest**:\n",
    "  - As described in Pipeline 3.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Frequency Resolution**: The length of the FFT determines the frequency resolution.\n",
    "- **Feature Selection**: Not all frequency components may be informative; selecting relevant frequencies can improve performance.\n",
    "\n",
    "---\n",
    "\n",
    "## 18. Time-Frequency Features + Linear Discriminant Analysis (LDA) (`TimeFreq_LDA`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Time-Frequency Analysis (e.g., STFT or Wavelet Transform)**:\n",
    "  - **Purpose**: Captures how the frequency content of the EEG signal changes over time.\n",
    "  - **How It Works**: Applies time-frequency transformations to extract features representing both temporal and spectral information.\n",
    "  - **Output**: Time-frequency representation flattened into feature vectors.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Linear Discriminant Analysis (LDA)**:\n",
    "  - As described in Pipeline 8.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Dimensionality Reduction**: Due to high dimensionality, techniques like PCA may be applied before classification.\n",
    "- **Temporal Dynamics**: Time-frequency features can capture transient patterns associated with mental imagery.\n",
    "\n",
    "---\n",
    "\n",
    "## 19. EEGNet (Deep Convolutional Neural Network) (`EEGNet_CNN`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: No external preprocessing; EEGNet handles preprocessing internally.\n",
    "\n",
    "### Feature Extraction and Classification\n",
    "\n",
    "- **EEGNet Architecture**:\n",
    "  - **Purpose**: A compact CNN architecture tailored for EEG-based BCIs.\n",
    "  - **How It Works**: Consists of convolutional layers that learn spatial and temporal filters, depthwise and separable convolutions to reduce parameters, and fully connected layers for classification.\n",
    "  - **Input Shape**: Expects input data in the shape `(n_samples, n_channels, n_times, 1)`.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **End-to-End Learning**: Learns feature extraction and classification jointly.\n",
    "- **Computational Resources**: Requires more computational power and training time compared to traditional methods.\n",
    "- **Data Requirements**: Deep learning models generally require large amounts of data to avoid overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## 20. PSD Features + XGBoost Classifier (`PSD_XGB`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Power Spectral Density (PSD)**:\n",
    "  - As described in Pipeline 2.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **XGBoost Classifier**:\n",
    "  - An optimized gradient boosting algorithm.\n",
    "  - **Advantages**: High performance, regularization to prevent overfitting, handles missing values.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Hyperparameter Tuning**: XGBoost has many parameters that can be tuned to optimize performance.\n",
    "- **Feature Importance**: Provides measures of feature importance, aiding in feature selection.\n",
    "\n",
    "---\n",
    "\n",
    "## 21. Ensemble Methods (Stacking Classifier) (`Stacking`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Power Spectral Density (PSD)**:\n",
    "  - As described in Pipeline 2.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Stacking Classifier**:\n",
    "  - **Base Learners**: Combines predictions from multiple classifiers (e.g., SVM, Random Forest, k-NN).\n",
    "  - **Meta-Learner**: Uses a higher-level classifier (e.g., Logistic Regression) to make the final prediction based on base learners' outputs.\n",
    "  - **Advantages**: Can capture diverse patterns by leveraging strengths of different classifiers.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Complexity**: Requires careful management to avoid overfitting due to increased model complexity.\n",
    "- **Cross-Validation**: Internal cross-validation is often used to prevent information leakage between training and validation data.\n",
    "\n",
    "---\n",
    "\n",
    "## 22. Sparse Representation Classification (SRC) (`SRC`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Raw Data or CSP Features**:\n",
    "  - **Purpose**: SRC can work with raw data or features extracted using methods like CSP.\n",
    "  - **How It Works**: Each class's training samples form a dictionary; test samples are represented as sparse linear combinations of dictionary atoms.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Sparse Representation Classifier (SRC)**:\n",
    "  - **How It Works**: Solves an optimization problem to find the sparsest representation of a test sample in terms of the training dictionary.\n",
    "  - **Prediction**: Classifies based on which class's dictionary best represents the test sample.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Computational Complexity**: SRC can be computationally intensive due to the optimization involved.\n",
    "- **Implementation**: In practice, approximations or simplifications (e.g., using Lasso regression) may be used.\n",
    "\n",
    "---\n",
    "\n",
    "## 23. Multilayer Perceptron (MLP) Neural Network (`MLP_NN`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Flattened EEG Data or Features**:\n",
    "  - The EEG data is reshaped into a 2D array (samples x features), either directly or after feature extraction.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **MLP Neural Network**:\n",
    "  - **Architecture**: Consists of input, hidden, and output layers with non-linear activation functions.\n",
    "  - **Learning**: Uses backpropagation to adjust weights and biases.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Flexibility**: MLPs can model complex non-linear relationships.\n",
    "- **Overfitting Risk**: Prone to overfitting if the network is too large or data is insufficient.\n",
    "- **Hyperparameters**: Number of layers, neurons, activation functions, and learning rate need to be tuned.\n",
    "\n",
    "---\n",
    "\n",
    "## General Considerations Across Pipelines\n",
    "\n",
    "- **Data Preprocessing**:\n",
    "  - **Bandpass Filtering**: Essential to focus on frequency bands relevant to mental imagery (e.g., alpha, beta rhythms).\n",
    "  - **Artifact Removal**: Techniques like ICA and CAR help reduce noise and artifacts, improving feature quality.\n",
    "\n",
    "- **Feature Extraction**:\n",
    "  - **Importance**: The choice of feature extraction method significantly impacts classification performance.\n",
    "  - **Dimensionality**: High-dimensional features may require dimensionality reduction or regularization to prevent overfitting.\n",
    "\n",
    "- **Classifier Selection**:\n",
    "  - **Linear vs. Non-linear**: Linear classifiers like LDA and Logistic Regression are simple and interpretable but may not capture complex patterns.\n",
    "  - **Ensemble Methods**: Can improve performance by combining multiple models but may increase computational cost.\n",
    "  - **Deep Learning Models**: Offer powerful feature learning capabilities but require more data and computational resources.\n",
    "\n",
    "- **Evaluation**:\n",
    "  - **Cross-Validation**: Essential for assessing model performance and generalization ability.\n",
    "  - **Hyperparameter Tuning**: Optimization of model parameters is crucial for achieving the best performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Each pipeline combines specific preprocessing techniques, feature extraction methods, and classifiers tailored to capture the characteristics of EEG signals associated with mental imagery tasks. The choice of pipeline depends on factors such as the nature of the EEG data, computational resources, and the specific requirements of the BCI application. By experimenting with different pipelines, researchers can identify the most effective approaches for their particular datasets and objectives.\n",
    "\n",
    "---\n",
    "\n",
    "If you have any questions about any of these pipelines or need further clarification on specific components, feel free to ask!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "from numpy import ndarray\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class LogVarianceTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Computes the log-variance of each channel over time.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit transformer. `y` is ignored.\"\"\"\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        \"\"\"Compute log-variance of the input data.\n",
    "\n",
    "        Parameters:\n",
    "        - X: array-like, shape (n_samples, n_channels, n_times)\n",
    "        \n",
    "        Returns:\n",
    "        - log_var: array, shape (n_samples, n_channels)\n",
    "        \"\"\"\n",
    "        assert X.ndim == 3, \"Input data X must be a 3D array.\"\n",
    "        log_var = np.log(np.var(X, axis=-1))\n",
    "        return log_var\n",
    "\n",
    "\n",
    "class LogVarianceTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Computes the log-variance of each channel.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        # X is of shape (n_samples, n_channels, n_times)\n",
    "        log_var = np.log(np.var(X, axis=2))\n",
    "        return log_var\n",
    "    \n",
    "class LogVariance(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"LogVariance transformer.\"\"\"\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"fit.\"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"transform.\"\"\"\n",
    "        assert X.ndim == 3\n",
    "        return np.log(np.var(X, -1))\n",
    "\n",
    "\n",
    "class FM(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Transformer to scale sampling frequency.\"\"\"\n",
    "\n",
    "    def __init__(self, freq=128):\n",
    "        \"\"\"Init function for FM transformer.\n",
    "\n",
    "        Instantaneous frequencies require a sampling frequency to be\n",
    "        properly scaled, which is helpful for some algorithms.\n",
    "\n",
    "        This assumes 128 if not told otherwise.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        freq: int\n",
    "            Sampling frequency of the signal. This is used to scale\n",
    "            the instantaneous frequency.\n",
    "        \"\"\"\n",
    "        self.freq = freq\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Only for scikit-learn compatibility.\"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"transform.\"\"\"\n",
    "        xphase = np.unwrap(np.angle(signal.hilbert(X, axis=-1)))\n",
    "        return np.median(self.freq * np.diff(xphase, axis=-1) / (2 * np.pi), axis=-1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
