{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of Pipelines for Mental Imagery BCI\n",
    "\n",
    "Below is a detailed explanation of each of the pipelines mentioned. For each pipeline, I'll describe:\n",
    "\n",
    "- **Preprocessing Steps**: Any data preprocessing or transformations applied before feature extraction.\n",
    "- **Feature Extraction**: Methods used to extract features from the EEG data.\n",
    "- **Classifier**: The machine learning algorithm used for classification.\n",
    "- **Notes**: Any additional information or considerations.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Common Spatial Patterns (CSP) + Logistic Regression (`CSP_LogReg`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Common Spatial Patterns (CSP)**:\n",
    "  - **Purpose**: CSP is a feature extraction method that projects multi-channel EEG data into a low-dimensional spatial subspace that maximizes the variance for one class while minimizing it for the other.\n",
    "  - **How It Works**: CSP computes spatial filters that optimize the discriminability between two classes by maximizing the variance of one class while minimizing the variance of the other.\n",
    "  - **Output**: CSP transforms the EEG data into a set of features (usually the log-variance of the filtered signals), resulting in a feature matrix suitable for classification.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Logistic Regression**:\n",
    "  - A linear classifier that models the probability of class membership using the logistic function.\n",
    "  - **Advantages**: Simple, interpretable, and performs well with linearly separable data.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **CSP Assumptions**: Works best when data is bandpass filtered to relevant frequency bands (e.g., mu and beta rhythms).\n",
    "- **Binary Classification**: CSP is typically used for binary classification tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Power Spectral Density (PSD) + Support Vector Machine (SVM) (`PSD_SVM`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Power Spectral Density (PSD)**:\n",
    "  - **Purpose**: PSD measures the power of the EEG signal at different frequency components.\n",
    "  - **How It Works**: Using methods like Welch's method or multitaper, the EEG data is transformed into the frequency domain, and the power at each frequency bin is calculated.\n",
    "  - **Output**: A feature vector representing the power distribution across frequencies for each channel.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Support Vector Machine (SVM)**:\n",
    "  - A supervised learning algorithm that finds the optimal hyperplane to separate classes.\n",
    "  - **Kernel Trick**: Can use different kernel functions (linear, polynomial, RBF) to handle non-linearly separable data.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Feature Dimensionality**: PSD features can be high-dimensional; dimensionality reduction or feature selection may be beneficial.\n",
    "- **SVM Advantages**: Effective in high-dimensional spaces and with clear margin separation.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Time Domain Features + Random Forest (`TimeDomain_RF`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Time-Domain Features**:\n",
    "  - **Mean**: Average value of the EEG signal over time.\n",
    "  - **Variance**: Measure of signal variability.\n",
    "  - **Skewness**: Measure of asymmetry in the signal distribution.\n",
    "  - **How It Works**: These statistical features are computed across the time dimension for each channel.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Random Forest**:\n",
    "  - An ensemble method using multiple decision trees.\n",
    "  - **How It Works**: Each tree is trained on a bootstrap sample with random feature selection; the final prediction is made by aggregating the outputs of all trees (e.g., majority vote).\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Non-linear Relationships**: Random forests can capture non-linear relationships between features and target classes.\n",
    "- **Feature Importance**: Random forests provide measures of feature importance, useful for feature selection.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Hilbert Transform + k-Nearest Neighbors (k-NN) (`Hilbert_KNN`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Hilbert Transform**:\n",
    "  - **Purpose**: To obtain the analytic signal from a real-valued signal, allowing extraction of instantaneous amplitude and phase.\n",
    "  - **How It Works**: The Hilbert transform is applied to the EEG signal to compute the amplitude envelope or phase information.\n",
    "  - **Output**: Amplitude envelope of the EEG signal for each channel.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **k-Nearest Neighbors (k-NN)**:\n",
    "  - A non-parametric classifier that assigns a class based on the majority class among the k nearest neighbors in the feature space.\n",
    "  - **Distance Metrics**: Commonly uses Euclidean distance, but other metrics can be applied.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Parameter Tuning**: The choice of `k` and the distance metric can significantly impact performance.\n",
    "- **Computational Cost**: k-NN can be computationally expensive for large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Wavelet Transform + PSD + Naive Bayes (`Wavelet_PSD_NB`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Wavelet Transform**:\n",
    "  - **Purpose**: Decomposes the EEG signal into time-frequency components with good time and frequency resolution.\n",
    "  - **How It Works**: Applies wavelet decomposition (e.g., using Daubechies wavelets) to capture signal characteristics at various scales.\n",
    "  - **Output**: Wavelet coefficients representing the signal at different scales and positions.\n",
    "- **Power Spectral Density (PSD)**:\n",
    "  - Calculated on the wavelet coefficients to obtain power distribution across scales.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Naive Bayes**:\n",
    "  - A probabilistic classifier based on Bayes' theorem, assuming feature independence.\n",
    "  - **Advantages**: Simple, fast, and works well with high-dimensional data.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Feature Independence Assumption**: Naive Bayes assumes features are independent, which may not hold for EEG data.\n",
    "- **Wavelet Selection**: The choice of wavelet function and decomposition level can affect feature quality.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Common Average Referencing (CAR) + CSP + Decision Tree (`CAR_CSP_DT`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Common Average Referencing (CAR)**:\n",
    "  - **Purpose**: To reduce common noise across all channels.\n",
    "  - **How It Works**: Subtracts the average signal across all channels from each channel's signal.\n",
    "  - **Output**: Referenced EEG data with reduced artifacts.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Common Spatial Patterns (CSP)**:\n",
    "  - As described in Pipeline 1.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Decision Tree**:\n",
    "  - A tree-based classifier that splits data based on feature thresholds to maximize class separation.\n",
    "  - **Advantages**: Simple to interpret, handles both numerical and categorical data.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Overfitting Risk**: Decision trees can overfit; pruning or setting maximum depth can mitigate this.\n",
    "- **CAR Benefits**: CAR can enhance signal-to-noise ratio, benefiting CSP feature extraction.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Independent Component Analysis (ICA) + Time Domain Features + SVM (`ICA_TimeDomain_SVM`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Independent Component Analysis (ICA)**:\n",
    "  - **Purpose**: To separate mixed signals into statistically independent components.\n",
    "  - **How It Works**: Decomposes EEG signals into independent sources, which can help isolate artifacts (e.g., eye blinks, muscle movements).\n",
    "  - **Output**: Cleaned EEG data or components representing neural activity.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Time-Domain Features**:\n",
    "  - As described in Pipeline 3.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Support Vector Machine (SVM)**:\n",
    "  - As described in Pipeline 2.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Artifact Removal**: ICA can effectively remove artifacts when components corresponding to noise are identified and excluded.\n",
    "- **Complexity**: ICA requires careful application to avoid removing neural signals of interest.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. CSP + Linear Discriminant Analysis (LDA) (`CSP_LDA`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Common Spatial Patterns (CSP)**:\n",
    "  - As described in Pipeline 1.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Linear Discriminant Analysis (LDA)**:\n",
    "  - A linear classifier that projects data onto a line to maximize class separation.\n",
    "  - **How It Works**: Finds a linear combination of features that best separates the classes.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Popular Combination**: CSP + LDA is a classic approach in motor imagery BCI applications due to its simplicity and effectiveness.\n",
    "- **Assumptions**: LDA assumes normally distributed features with equal covariance matrices for each class.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. PSD + Gradient Boosting Classifier (`PSD_GB`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Power Spectral Density (PSD)**:\n",
    "  - As described in Pipeline 2.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Gradient Boosting Classifier**:\n",
    "  - An ensemble method that builds sequential weak learners (typically decision trees), where each new learner focuses on correcting the errors of previous ones.\n",
    "  - **Advantages**: High performance, can model complex relationships.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Hyperparameter Tuning**: Requires careful tuning of parameters like learning rate, number of estimators, and tree depth.\n",
    "- **Overfitting Risk**: Can overfit if not properly regularized.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. CAR + Riemannian Geometry Features + Logistic Regression (`CAR_Riemann_LogReg`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Common Average Referencing (CAR)**:\n",
    "  - As described in Pipeline 6.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Riemannian Geometry Features**:\n",
    "  - **Covariance Matrices**: Compute the covariance matrix of EEG signals for each trial.\n",
    "  - **Tangent Space Mapping**: Maps covariance matrices to the tangent space of the Riemannian manifold to obtain feature vectors.\n",
    "  - **Purpose**: Captures the spatial covariance structure of EEG signals, which is informative for classification.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Logistic Regression**:\n",
    "  - As described in Pipeline 1.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Riemannian Methods**: Effective for BCI applications due to robustness to noise and ability to capture complex signal structures.\n",
    "- **Computational Cost**: Calculating covariance matrices and tangent space mapping can be computationally intensive.\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Short-Time Fourier Transform (STFT) + SVM (`STFT_SVM`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Short-Time Fourier Transform (STFT)**:\n",
    "  - **Purpose**: Analyzes the signal's frequency content over time by applying the Fourier Transform to short overlapping time windows.\n",
    "  - **How It Works**: Divides the EEG signal into segments, applies windowing, and computes the Fourier Transform for each segment.\n",
    "  - **Output**: Time-frequency representation of the signal.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Support Vector Machine (SVM)**:\n",
    "  - As described in Pipeline 2.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Feature Dimensionality**: STFT features can be very high-dimensional; dimensionality reduction may be necessary.\n",
    "- **Time-Frequency Analysis**: Useful for capturing non-stationary properties of EEG signals.\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Laplacian Spatial Filtering + Wavelet Packet Decomposition (WPD) + Random Forest (`Laplacian_WPD_RF`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Laplacian Spatial Filtering**:\n",
    "  - **Purpose**: Enhances spatial resolution by emphasizing local activity and reducing volume conduction effects.\n",
    "  - **How It Works**: Computes the difference between a channel and the average of its neighboring channels.\n",
    "  - **Output**: Spatially filtered EEG data.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Wavelet Packet Decomposition (WPD)**:\n",
    "  - **Purpose**: Decomposes the signal into a set of frequency subbands with both time and frequency localization.\n",
    "  - **How It Works**: Applies wavelet packet analysis to the EEG data, providing a more detailed frequency analysis than standard wavelet transforms.\n",
    "  - **Output**: Coefficients representing the signal's energy at various scales and positions.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Random Forest**:\n",
    "  - As described in Pipeline 3.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Information Preservation**: WPD can capture subtle signal features that may be relevant for classification.\n",
    "- **Computational Complexity**: WPD can be computationally intensive due to the extensive decomposition.\n",
    "\n",
    "---\n",
    "\n",
    "## 13. Morlet Wavelets + k-NN (`Morlet_KNN`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Time-Frequency Analysis using Morlet Wavelets**:\n",
    "  - **Purpose**: Captures time-frequency characteristics of EEG signals with high resolution.\n",
    "  - **How It Works**: Applies Morlet wavelet transform, which provides a complex exponential modulated by a Gaussian, ideal for EEG analysis.\n",
    "  - **Output**: Power spectra across frequencies and time.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **k-Nearest Neighbors (k-NN)**:\n",
    "  - As described in Pipeline 4.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Frequency Selection**: Frequencies of interest (e.g., 8-30 Hz) are analyzed.\n",
    "- **Data Dimensionality**: The resulting features may be high-dimensional, affecting computational cost.\n",
    "\n",
    "---\n",
    "\n",
    "## 14. Autoregressive (AR) Coefficients + Logistic Regression (`AR_LogReg`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Autoregressive (AR) Coefficients**:\n",
    "  - **Purpose**: Models the EEG signal as a linear function of its previous values.\n",
    "  - **How It Works**: Fits an AR model to the EEG time series, extracting the coefficients as features.\n",
    "  - **Output**: AR coefficients for each channel.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Logistic Regression**:\n",
    "  - As described in Pipeline 1.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Model Order**: The choice of AR model order (number of lags) affects feature quality.\n",
    "- **Stationarity Assumption**: AR models assume signal stationarity within the analysis window.\n",
    "\n",
    "---\n",
    "\n",
    "## 15. Mean Amplitude Features + Naive Bayes (`MeanAmp_NB`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Mean Amplitude**:\n",
    "  - **Purpose**: Simple feature representing the average absolute value of the EEG signal.\n",
    "  - **How It Works**: Computes the mean of the absolute values of the EEG signal over time for each channel.\n",
    "  - **Output**: Feature vector of mean amplitudes.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Naive Bayes**:\n",
    "  - As described in Pipeline 5.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Simplicity**: This approach uses straightforward features and a simple classifier.\n",
    "- **Performance**: May not capture complex signal characteristics, potentially limiting classification accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## 16. ICA + PSD + SVM (`ICA_PSD_SVM`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Independent Component Analysis (ICA)**:\n",
    "  - As described in Pipeline 7.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Power Spectral Density (PSD)**:\n",
    "  - Calculated on the ICA components to analyze the frequency content of the independent sources.\n",
    "  - **Output**: PSD features derived from the independent components.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Support Vector Machine (SVM)**:\n",
    "  - As described in Pipeline 2.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Artifact Removal**: ICA can help isolate neural signals from artifacts before feature extraction.\n",
    "- **Component Selection**: Choosing relevant components is crucial; including artifact components can degrade performance.\n",
    "\n",
    "---\n",
    "\n",
    "## 17. Fast Fourier Transform (FFT) + Random Forest (`FFT_RF`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Fast Fourier Transform (FFT)**:\n",
    "  - **Purpose**: Transforms the time-domain EEG signal into the frequency domain.\n",
    "  - **How It Works**: Computes the discrete Fourier Transform efficiently to obtain frequency coefficients.\n",
    "  - **Output**: Magnitude of the FFT coefficients for each channel.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Random Forest**:\n",
    "  - As described in Pipeline 3.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Frequency Resolution**: The length of the FFT determines the frequency resolution.\n",
    "- **Feature Selection**: Not all frequency components may be informative; selecting relevant frequencies can improve performance.\n",
    "\n",
    "---\n",
    "\n",
    "## 18. Time-Frequency Features + Linear Discriminant Analysis (LDA) (`TimeFreq_LDA`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Time-Frequency Analysis (e.g., STFT or Wavelet Transform)**:\n",
    "  - **Purpose**: Captures how the frequency content of the EEG signal changes over time.\n",
    "  - **How It Works**: Applies time-frequency transformations to extract features representing both temporal and spectral information.\n",
    "  - **Output**: Time-frequency representation flattened into feature vectors.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Linear Discriminant Analysis (LDA)**:\n",
    "  - As described in Pipeline 8.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Dimensionality Reduction**: Due to high dimensionality, techniques like PCA may be applied before classification.\n",
    "- **Temporal Dynamics**: Time-frequency features can capture transient patterns associated with mental imagery.\n",
    "\n",
    "---\n",
    "\n",
    "## 19. EEGNet (Deep Convolutional Neural Network) (`EEGNet_CNN`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: No external preprocessing; EEGNet handles preprocessing internally.\n",
    "\n",
    "### Feature Extraction and Classification\n",
    "\n",
    "- **EEGNet Architecture**:\n",
    "  - **Purpose**: A compact CNN architecture tailored for EEG-based BCIs.\n",
    "  - **How It Works**: Consists of convolutional layers that learn spatial and temporal filters, depthwise and separable convolutions to reduce parameters, and fully connected layers for classification.\n",
    "  - **Input Shape**: Expects input data in the shape `(n_samples, n_channels, n_times, 1)`.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **End-to-End Learning**: Learns feature extraction and classification jointly.\n",
    "- **Computational Resources**: Requires more computational power and training time compared to traditional methods.\n",
    "- **Data Requirements**: Deep learning models generally require large amounts of data to avoid overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## 20. PSD Features + XGBoost Classifier (`PSD_XGB`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Power Spectral Density (PSD)**:\n",
    "  - As described in Pipeline 2.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **XGBoost Classifier**:\n",
    "  - An optimized gradient boosting algorithm.\n",
    "  - **Advantages**: High performance, regularization to prevent overfitting, handles missing values.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Hyperparameter Tuning**: XGBoost has many parameters that can be tuned to optimize performance.\n",
    "- **Feature Importance**: Provides measures of feature importance, aiding in feature selection.\n",
    "\n",
    "---\n",
    "\n",
    "## 21. Ensemble Methods (Stacking Classifier) (`Stacking`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Power Spectral Density (PSD)**:\n",
    "  - As described in Pipeline 2.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Stacking Classifier**:\n",
    "  - **Base Learners**: Combines predictions from multiple classifiers (e.g., SVM, Random Forest, k-NN).\n",
    "  - **Meta-Learner**: Uses a higher-level classifier (e.g., Logistic Regression) to make the final prediction based on base learners' outputs.\n",
    "  - **Advantages**: Can capture diverse patterns by leveraging strengths of different classifiers.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Complexity**: Requires careful management to avoid overfitting due to increased model complexity.\n",
    "- **Cross-Validation**: Internal cross-validation is often used to prevent information leakage between training and validation data.\n",
    "\n",
    "---\n",
    "\n",
    "## 22. Sparse Representation Classification (SRC) (`SRC`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Raw Data or CSP Features**:\n",
    "  - **Purpose**: SRC can work with raw data or features extracted using methods like CSP.\n",
    "  - **How It Works**: Each class's training samples form a dictionary; test samples are represented as sparse linear combinations of dictionary atoms.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **Sparse Representation Classifier (SRC)**:\n",
    "  - **How It Works**: Solves an optimization problem to find the sparsest representation of a test sample in terms of the training dictionary.\n",
    "  - **Prediction**: Classifies based on which class's dictionary best represents the test sample.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Computational Complexity**: SRC can be computationally intensive due to the optimization involved.\n",
    "- **Implementation**: In practice, approximations or simplifications (e.g., using Lasso regression) may be used.\n",
    "\n",
    "---\n",
    "\n",
    "## 23. Multilayer Perceptron (MLP) Neural Network (`MLP_NN`)\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Assumption**: Bandpass filtering is performed separately before this pipeline.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "- **Flattened EEG Data or Features**:\n",
    "  - The EEG data is reshaped into a 2D array (samples x features), either directly or after feature extraction.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "- **MLP Neural Network**:\n",
    "  - **Architecture**: Consists of input, hidden, and output layers with non-linear activation functions.\n",
    "  - **Learning**: Uses backpropagation to adjust weights and biases.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Flexibility**: MLPs can model complex non-linear relationships.\n",
    "- **Overfitting Risk**: Prone to overfitting if the network is too large or data is insufficient.\n",
    "- **Hyperparameters**: Number of layers, neurons, activation functions, and learning rate need to be tuned.\n",
    "\n",
    "---\n",
    "\n",
    "## General Considerations Across Pipelines\n",
    "\n",
    "- **Data Preprocessing**:\n",
    "  - **Bandpass Filtering**: Essential to focus on frequency bands relevant to mental imagery (e.g., alpha, beta rhythms).\n",
    "  - **Artifact Removal**: Techniques like ICA and CAR help reduce noise and artifacts, improving feature quality.\n",
    "\n",
    "- **Feature Extraction**:\n",
    "  - **Importance**: The choice of feature extraction method significantly impacts classification performance.\n",
    "  - **Dimensionality**: High-dimensional features may require dimensionality reduction or regularization to prevent overfitting.\n",
    "\n",
    "- **Classifier Selection**:\n",
    "  - **Linear vs. Non-linear**: Linear classifiers like LDA and Logistic Regression are simple and interpretable but may not capture complex patterns.\n",
    "  - **Ensemble Methods**: Can improve performance by combining multiple models but may increase computational cost.\n",
    "  - **Deep Learning Models**: Offer powerful feature learning capabilities but require more data and computational resources.\n",
    "\n",
    "- **Evaluation**:\n",
    "  - **Cross-Validation**: Essential for assessing model performance and generalization ability.\n",
    "  - **Hyperparameter Tuning**: Optimization of model parameters is crucial for achieving the best performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Each pipeline combines specific preprocessing techniques, feature extraction methods, and classifiers tailored to capture the characteristics of EEG signals associated with mental imagery tasks. The choice of pipeline depends on factors such as the nature of the EEG data, computational resources, and the specific requirements of the BCI application. By experimenting with different pipelines, researchers can identify the most effective approaches for their particular datasets and objectives.\n",
    "\n",
    "---\n",
    "\n",
    "If you have any questions about any of these pipelines or need further clarification on specific components, feel free to ask!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('c:\\\\Users\\\\rokas\\\\Documents\\\\BCI\\\\mi-bci\\\\code')\n",
    "\n",
    "# Import ml_pipelines from the file you saved earlier\n",
    "from pipelines.ml_pipelines import ml_pipelines\n",
    "from evaluation import train_and_evaluate\n",
    "\n",
    "from helper_functions import setup_logger, load_procesed_data\n",
    "from helper_functions import process_mi_epochs\n",
    "from datasets import Lee2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\data\\procesed\\20\\MI\\44\\1\\s44.01_epochs_raw-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1200.00 ...    5200.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "100 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\data\\procesed\\20\\MI\\44\\1\\s44.01_epochs_raw_autoreject-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1200.00 ...    5200.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "73 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Setting up band-pass filter from 8 - 30 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandpass zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 20 (effective, after forward-backward)\n",
      "- Cutoffs at 8.00, 30.00 Hz: -6.02, -6.02 dB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log = setup_logger(\"Lee_preprocess\")\n",
    "dataset = Lee2019()\n",
    "dataset_no = 20\n",
    "paradigm = \"MI\"\n",
    "subject = 44\n",
    "run = 1\n",
    "data = load_procesed_data(dataset_no, paradigm, subject, run, include=['epochs_raw', 'epochs_raw_autoreject'])\n",
    "epochs = data[\"epochs_raw\"]\n",
    "epochs_p = process_mi_epochs(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating pipeline: CSP_LogReg\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.7e-05 (2.2e-16 eps * 20 dim * 6.1e+09  max singular value)\n",
      "    Estimated rank (data): 20\n",
      "    data: rank 20 computed from 20 data channels with 0 projectors\n",
      "Reducing data rank from 20 -> 20\n",
      "Estimating class=0 covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating class=1 covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.7e-05 (2.2e-16 eps * 20 dim * 6e+09  max singular value)\n",
      "    Estimated rank (data): 20\n",
      "    data: rank 20 computed from 20 data channels with 0 projectors\n",
      "Reducing data rank from 20 -> 20\n",
      "Estimating class=0 covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating class=1 covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.8e-05 (2.2e-16 eps * 20 dim * 6.2e+09  max singular value)\n",
      "    Estimated rank (data): 20\n",
      "    data: rank 20 computed from 20 data channels with 0 projectors\n",
      "Reducing data rank from 20 -> 20\n",
      "Estimating class=0 covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating class=1 covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.8e-05 (2.2e-16 eps * 20 dim * 6.2e+09  max singular value)\n",
      "    Estimated rank (data): 20\n",
      "    data: rank 20 computed from 20 data channels with 0 projectors\n",
      "Reducing data rank from 20 -> 20\n",
      "Estimating class=0 covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating class=1 covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.7e-05 (2.2e-16 eps * 20 dim * 6.1e+09  max singular value)\n",
      "    Estimated rank (data): 20\n",
      "    data: rank 20 computed from 20 data channels with 0 projectors\n",
      "Reducing data rank from 20 -> 20\n",
      "Estimating class=0 covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating class=1 covariance using EMPIRICAL\n",
      "Done.\n",
      "======= Fold 0 =======\n",
      "Training Accuracy: 0.9875, Training AUC: 0.9956\n",
      "Validation Accuracy: 0.9000, Validation AUC: 0.9700\n",
      "======= Fold 1 =======\n",
      "Training Accuracy: 0.9750, Training AUC: 0.9931\n",
      "Validation Accuracy: 0.9500, Validation AUC: 0.9900\n",
      "======= Fold 2 =======\n",
      "Training Accuracy: 0.9750, Training AUC: 0.9912\n",
      "Validation Accuracy: 0.9500, Validation AUC: 1.0000\n",
      "======= Fold 3 =======\n",
      "Training Accuracy: 0.9875, Training AUC: 0.9950\n",
      "Validation Accuracy: 0.9500, Validation AUC: 0.9900\n",
      "======= Fold 4 =======\n",
      "Training Accuracy: 0.9750, Training AUC: 0.9950\n",
      "Validation Accuracy: 0.9000, Validation AUC: 1.0000\n",
      "Our out-of-fold mean training accuracy is 0.9800\n",
      "Our out-of-fold mean training AUC is 0.9940\n",
      "Our out-of-fold mean validation accuracy is 0.9300\n",
      "Our out-of-fold mean validation AUC is 0.9900\n",
      "Evaluating pipeline: PSD_SVM\n",
      "======= Fold 0 =======\n",
      "Training Accuracy: 0.9375, Training AUC: 0.9900\n",
      "Validation Accuracy: 0.4000, Validation AUC: 0.4700\n",
      "======= Fold 1 =======\n",
      "Training Accuracy: 0.8750, Training AUC: 0.9531\n",
      "Validation Accuracy: 0.6000, Validation AUC: 0.6600\n",
      "======= Fold 2 =======\n",
      "Training Accuracy: 0.9250, Training AUC: 0.9712\n",
      "Validation Accuracy: 0.6000, Validation AUC: 0.6000\n",
      "======= Fold 3 =======\n",
      "Training Accuracy: 0.9000, Training AUC: 0.9719\n",
      "Validation Accuracy: 0.5000, Validation AUC: 0.6800\n",
      "======= Fold 4 =======\n",
      "Training Accuracy: 0.8875, Training AUC: 0.9819\n",
      "Validation Accuracy: 0.6500, Validation AUC: 0.6200\n",
      "Our out-of-fold mean training accuracy is 0.9050\n",
      "Our out-of-fold mean training AUC is 0.9736\n",
      "Our out-of-fold mean validation accuracy is 0.5500\n",
      "Our out-of-fold mean validation AUC is 0.6060\n",
      "Evaluating pipeline: TimeDomain_RF\n",
      "======= Fold 0 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.6000, Validation AUC: 0.6000\n",
      "======= Fold 1 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.6000, Validation AUC: 0.5800\n",
      "======= Fold 2 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.6500, Validation AUC: 0.6400\n",
      "======= Fold 3 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.6000, Validation AUC: 0.6500\n",
      "======= Fold 4 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.5000, Validation AUC: 0.5100\n",
      "Our out-of-fold mean training accuracy is 1.0000\n",
      "Our out-of-fold mean training AUC is 1.0000\n",
      "Our out-of-fold mean validation accuracy is 0.5900\n",
      "Our out-of-fold mean validation AUC is 0.5960\n",
      "Evaluating pipeline: Hilbert_KNN\n",
      "An error occurred in pipeline Hilbert_KNN: \n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\pipeline.py\", line 469, in fit\n",
      "    Xt = self._fit(X, y, routed_params)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\pipeline.py\", line 406, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\pipeline.py\", line 1310, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\base.py\", line 1101, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py\", line 61, in transform\n",
      "    analytic_signal = mne.filter.hilbert(X, picks=None, envelope=False, verbose=False)\n",
      "AttributeError: module 'mne.filter' has no attribute 'hilbert'\n",
      "\n",
      "Evaluating pipeline: Wavelet_PSD_NB\n",
      "An error occurred in pipeline Wavelet_PSD_NB: \n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\pipeline.py\", line 469, in fit\n",
      "    Xt = self._fit(X, y, routed_params)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\pipeline.py\", line 406, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\pipeline.py\", line 1310, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\base.py\", line 1101, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py\", line 70, in transform\n",
      "    import pywt\n",
      "ModuleNotFoundError: No module named 'pywt'\n",
      "\n",
      "Evaluating pipeline: CAR_CSP_DT\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.1e-05 (2.2e-16 eps * 20 dim * 2.4e+09  max singular value)\n",
      "    Estimated rank (data): 19\n",
      "    data: rank 19 computed from 20 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 20 -> 19\n",
      "Estimating class=0 covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating class=1 covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.1e-05 (2.2e-16 eps * 20 dim * 2.4e+09  max singular value)\n",
      "    Estimated rank (data): 20\n",
      "    data: rank 20 computed from 20 data channels with 0 projectors\n",
      "Reducing data rank from 20 -> 20\n",
      "Estimating class=0 covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating class=1 covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.1e-05 (2.2e-16 eps * 20 dim * 2.5e+09  max singular value)\n",
      "    Estimated rank (data): 19\n",
      "    data: rank 19 computed from 20 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 20 -> 19\n",
      "Estimating class=0 covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating class=1 covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.1e-05 (2.2e-16 eps * 20 dim * 2.5e+09  max singular value)\n",
      "    Estimated rank (data): 19\n",
      "    data: rank 19 computed from 20 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 20 -> 19\n",
      "Estimating class=0 covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating class=1 covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.1e-05 (2.2e-16 eps * 20 dim * 2.5e+09  max singular value)\n",
      "    Estimated rank (data): 19\n",
      "    data: rank 19 computed from 20 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 20 -> 19\n",
      "Estimating class=0 covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating class=1 covariance using EMPIRICAL\n",
      "Done.\n",
      "======= Fold 0 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.9000, Validation AUC: 0.9000\n",
      "======= Fold 1 =======\n",
      "Training Accuracy: nan, Training AUC: nan\n",
      "Validation Accuracy: nan, Validation AUC: nan\n",
      "======= Fold 2 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 1.0000, Validation AUC: 1.0000\n",
      "======= Fold 3 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.9500, Validation AUC: 0.9500\n",
      "======= Fold 4 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.9500, Validation AUC: 0.9500\n",
      "Our out-of-fold mean training accuracy is nan\n",
      "Our out-of-fold mean training AUC is nan\n",
      "Our out-of-fold mean validation accuracy is nan\n",
      "Our out-of-fold mean validation AUC is nan\n",
      "Evaluating pipeline: ICA_TimeDomain_SVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "1 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\pipeline.py\", line 469, in fit\n",
      "    Xt = self._fit(X, y, routed_params)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\pipeline.py\", line 406, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\pipeline.py\", line 1310, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\mne\\decoding\\csp.py\", line 277, in fit_transform\n",
      "    return super().fit_transform(X, y=y, **fit_params)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\mne\\decoding\\mixin.py\", line 36, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\mne\\decoding\\csp.py\", line 192, in fit\n",
      "    eigen_vectors, eigen_values = self._decompose_covs(covs, sample_weights)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\mne\\decoding\\csp.py\", line 636, in _decompose_covs\n",
      "    eigen_values, eigen_vectors = eigh(covs[0], covs.sum(0))\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\scipy\\linalg\\_decomp.py\", line 592, in eigh\n",
      "    raise LinAlgError(f'The leading minor of order {info-n} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 20 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py:102: RuntimeWarning: Channel names are not unique, found duplicates for: {'eeg'}. Applying running numbers for duplicates.\n",
      "  info = mne.create_info(ch_names=['eeg'] * X_concat.shape[0],\n",
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py:105: RuntimeWarning: The data has not been high-pass filtered. For good ICA performance, it should be high-pass filtered (e.g., with a 1.0 Hz lower bound) before fitting ICA.\n",
      "  self.ica.fit(raw)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ICA to data using 1600 channels (please be patient, this may take a while)\n",
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.3s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py:112: RuntimeWarning: Channel names are not unique, found duplicates for: {'eeg'}. Applying running numbers for duplicates.\n",
      "  info = mne.create_info(ch_names=['eeg'] * n_channels,\n",
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py:102: RuntimeWarning: Channel names are not unique, found duplicates for: {'eeg'}. Applying running numbers for duplicates.\n",
      "  info = mne.create_info(ch_names=['eeg'] * X_concat.shape[0],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ICA to data using 1600 channels (please be patient, this may take a while)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py:105: RuntimeWarning: The data has not been high-pass filtered. For good ICA performance, it should be high-pass filtered (e.g., with a 1.0 Hz lower bound) before fitting ICA.\n",
      "  self.ica.fit(raw)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.6s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py:112: RuntimeWarning: Channel names are not unique, found duplicates for: {'eeg'}. Applying running numbers for duplicates.\n",
      "  info = mne.create_info(ch_names=['eeg'] * n_channels,\n",
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py:102: RuntimeWarning: Channel names are not unique, found duplicates for: {'eeg'}. Applying running numbers for duplicates.\n",
      "  info = mne.create_info(ch_names=['eeg'] * X_concat.shape[0],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ICA to data using 1600 channels (please be patient, this may take a while)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py:105: RuntimeWarning: The data has not been high-pass filtered. For good ICA performance, it should be high-pass filtered (e.g., with a 1.0 Hz lower bound) before fitting ICA.\n",
      "  self.ica.fit(raw)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.2s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py:112: RuntimeWarning: Channel names are not unique, found duplicates for: {'eeg'}. Applying running numbers for duplicates.\n",
      "  info = mne.create_info(ch_names=['eeg'] * n_channels,\n",
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py:102: RuntimeWarning: Channel names are not unique, found duplicates for: {'eeg'}. Applying running numbers for duplicates.\n",
      "  info = mne.create_info(ch_names=['eeg'] * X_concat.shape[0],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ICA to data using 1600 channels (please be patient, this may take a while)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py:105: RuntimeWarning: The data has not been high-pass filtered. For good ICA performance, it should be high-pass filtered (e.g., with a 1.0 Hz lower bound) before fitting ICA.\n",
      "  self.ica.fit(raw)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.2s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py:112: RuntimeWarning: Channel names are not unique, found duplicates for: {'eeg'}. Applying running numbers for duplicates.\n",
      "  info = mne.create_info(ch_names=['eeg'] * n_channels,\n",
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py:102: RuntimeWarning: Channel names are not unique, found duplicates for: {'eeg'}. Applying running numbers for duplicates.\n",
      "  info = mne.create_info(ch_names=['eeg'] * X_concat.shape[0],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ICA to data using 1600 channels (please be patient, this may take a while)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py:105: RuntimeWarning: The data has not been high-pass filtered. For good ICA performance, it should be high-pass filtered (e.g., with a 1.0 Hz lower bound) before fitting ICA.\n",
      "  self.ica.fit(raw)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.1s.\n",
      "An error occurred in pipeline ICA_TimeDomain_SVM: \n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\pipeline.py\", line 469, in fit\n",
      "    Xt = self._fit(X, y, routed_params)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\pipeline.py\", line 406, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\pipeline.py\", line 1310, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\base.py\", line 1101, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py\", line 115, in transform\n",
      "    raw_ica = self.ica.apply(raw.copy(), exclude=[], verbose=False)\n",
      "  File \"<decorator-gen-413>\", line 10, in apply\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\mne\\preprocessing\\ica.py\", line 2237, in apply\n",
      "    out = meth(**kwargs)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\mne\\preprocessing\\ica.py\", line 2260, in _apply_raw\n",
      "    data = self._pick_sources(data, include, exclude, n_pca_components)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\mne\\preprocessing\\ica.py\", line 2318, in _pick_sources\n",
      "    data = self._pre_whiten(data)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\mne\\preprocessing\\ica.py\", line 885, in _pre_whiten\n",
      "    data /= self.pre_whitener_\n",
      "ValueError: operands could not be broadcast together with shapes (20,2501) (1600,1) (20,2501) \n",
      "\n",
      "Evaluating pipeline: CSP_LDA\n",
      "Computing rank from data with rank=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py:112: RuntimeWarning: Channel names are not unique, found duplicates for: {'eeg'}. Applying running numbers for duplicates.\n",
      "  info = mne.create_info(ch_names=['eeg'] * n_channels,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Using tolerance 2.7e-05 (2.2e-16 eps * 20 dim * 6.1e+09  max singular value)\n",
      "    Estimated rank (data): 20\n",
      "    data: rank 20 computed from 20 data channels with 0 projectors\n",
      "Reducing data rank from 20 -> 20\n",
      "Estimating class=0 covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating class=1 covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.7e-05 (2.2e-16 eps * 20 dim * 6e+09  max singular value)\n",
      "    Estimated rank (data): 20\n",
      "    data: rank 20 computed from 20 data channels with 0 projectors\n",
      "Reducing data rank from 20 -> 20\n",
      "Estimating class=0 covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating class=1 covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.8e-05 (2.2e-16 eps * 20 dim * 6.2e+09  max singular value)\n",
      "    Estimated rank (data): 20\n",
      "    data: rank 20 computed from 20 data channels with 0 projectors\n",
      "Reducing data rank from 20 -> 20\n",
      "Estimating class=0 covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating class=1 covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.8e-05 (2.2e-16 eps * 20 dim * 6.2e+09  max singular value)\n",
      "    Estimated rank (data): 20\n",
      "    data: rank 20 computed from 20 data channels with 0 projectors\n",
      "Reducing data rank from 20 -> 20\n",
      "Estimating class=0 covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating class=1 covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.7e-05 (2.2e-16 eps * 20 dim * 6.1e+09  max singular value)\n",
      "    Estimated rank (data): 20\n",
      "    data: rank 20 computed from 20 data channels with 0 projectors\n",
      "Reducing data rank from 20 -> 20\n",
      "Estimating class=0 covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating class=1 covariance using EMPIRICAL\n",
      "Done.\n",
      "======= Fold 0 =======\n",
      "Training Accuracy: 0.9875, Training AUC: 0.9975\n",
      "Validation Accuracy: 0.9000, Validation AUC: 0.9700\n",
      "======= Fold 1 =======\n",
      "Training Accuracy: 0.9750, Training AUC: 0.9912\n",
      "Validation Accuracy: 0.9000, Validation AUC: 0.9800\n",
      "======= Fold 2 =======\n",
      "Training Accuracy: 0.9375, Training AUC: 0.9913\n",
      "Validation Accuracy: 0.9500, Validation AUC: 0.9900\n",
      "======= Fold 3 =======\n",
      "Training Accuracy: 0.9875, Training AUC: 0.9962\n",
      "Validation Accuracy: 0.9500, Validation AUC: 0.9900\n",
      "======= Fold 4 =======\n",
      "Training Accuracy: 0.9625, Training AUC: 0.9938\n",
      "Validation Accuracy: 0.9000, Validation AUC: 1.0000\n",
      "Our out-of-fold mean training accuracy is 0.9700\n",
      "Our out-of-fold mean training AUC is 0.9940\n",
      "Our out-of-fold mean validation accuracy is 0.9200\n",
      "Our out-of-fold mean validation AUC is 0.9860\n",
      "Evaluating pipeline: PSD_GB\n",
      "======= Fold 0 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.7000, Validation AUC: 0.7450\n",
      "======= Fold 1 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.8500, Validation AUC: 0.9600\n",
      "======= Fold 2 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.8500, Validation AUC: 0.8350\n",
      "======= Fold 3 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.8000, Validation AUC: 0.8750\n",
      "======= Fold 4 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.7000, Validation AUC: 0.7100\n",
      "Our out-of-fold mean training accuracy is 1.0000\n",
      "Our out-of-fold mean training AUC is 1.0000\n",
      "Our out-of-fold mean validation accuracy is 0.7800\n",
      "Our out-of-fold mean validation AUC is 0.8250\n",
      "Evaluating pipeline: CAR_Riemann_LogReg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\pyriemann\\utils\\base.py:18: RuntimeWarning: invalid value encountered in log\n",
      "  eigvals = operator(eigvals)\n",
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\pyriemann\\utils\\base.py:18: RuntimeWarning: invalid value encountered in sqrt\n",
      "  eigvals = operator(eigvals)\n",
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\pyriemann\\utils\\base.py:73: RuntimeWarning: invalid value encountered in sqrt\n",
      "  def isqrt(x): return 1. / np.sqrt(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred in pipeline CAR_Riemann_LogReg: \n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\pipeline.py\", line 469, in fit\n",
      "    Xt = self._fit(X, y, routed_params)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\pipeline.py\", line 406, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\pipeline.py\", line 1310, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\pyriemann\\tangentspace.py\", line 183, in fit_transform\n",
      "    self.reference_ = mean_covariance(\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\pyriemann\\utils\\mean.py\", line 785, in mean_covariance\n",
      "    M = mean_function(\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\pyriemann\\utils\\mean.py\", line 647, in mean_riemann\n",
      "    M = M12 @ expm(nu * J) @ M12\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\pyriemann\\utils\\base.py\", line 47, in expm\n",
      "    return _matrix_operator(C, np.exp)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\pyriemann\\utils\\base.py\", line 14, in _matrix_operator\n",
      "    raise ValueError(\n",
      "ValueError: Matrices must be positive definite. Add regularization to avoid this error.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\pipeline.py\", line 469, in fit\n",
      "    Xt = self._fit(X, y, routed_params)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\pipeline.py\", line 406, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\pipeline.py\", line 1310, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\pyriemann\\tangentspace.py\", line 183, in fit_transform\n",
      "    self.reference_ = mean_covariance(\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\pyriemann\\utils\\mean.py\", line 785, in mean_covariance\n",
      "    M = mean_function(\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\pyriemann\\utils\\mean.py\", line 646, in mean_riemann\n",
      "    J = np.einsum(\"a,abc->bc\", sample_weight, logm(Mm12 @ X @ Mm12))\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\pyriemann\\utils\\base.py\", line 99, in logm\n",
      "    return _matrix_operator(C, np.log)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\pyriemann\\utils\\base.py\", line 14, in _matrix_operator\n",
      "    raise ValueError(\n",
      "ValueError: Matrices must be positive definite. Add regularization to avoid this error.\n",
      "\n",
      "Evaluating pipeline: STFT_SVM\n",
      "======= Fold 0 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.7000, Validation AUC: 0.6900\n",
      "======= Fold 1 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.7000, Validation AUC: 0.7200\n",
      "======= Fold 2 =======\n",
      "Training Accuracy: 0.9875, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.5500, Validation AUC: 0.6800\n",
      "======= Fold 3 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.6500, Validation AUC: 0.7700\n",
      "======= Fold 4 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.4500, Validation AUC: 0.4700\n",
      "Our out-of-fold mean training accuracy is 0.9975\n",
      "Our out-of-fold mean training AUC is 1.0000\n",
      "Our out-of-fold mean validation accuracy is 0.6100\n",
      "Our out-of-fold mean validation AUC is 0.6660\n",
      "Evaluating pipeline: Morlet_KNN\n",
      "======= Fold 0 =======\n",
      "Training Accuracy: 0.7875, Training AUC: 0.8259\n",
      "Validation Accuracy: 0.7500, Validation AUC: 0.6850\n",
      "======= Fold 1 =======\n",
      "Training Accuracy: 0.7500, Training AUC: 0.8634\n",
      "Validation Accuracy: 0.5500, Validation AUC: 0.5500\n",
      "======= Fold 2 =======\n",
      "Training Accuracy: 0.8125, Training AUC: 0.8497\n",
      "Validation Accuracy: 0.3500, Validation AUC: 0.3850\n",
      "======= Fold 3 =======\n",
      "Training Accuracy: 0.8250, Training AUC: 0.8444\n",
      "Validation Accuracy: 0.6500, Validation AUC: 0.7450\n",
      "======= Fold 4 =======\n",
      "Training Accuracy: 0.6375, Training AUC: 0.7925\n",
      "Validation Accuracy: 0.6000, Validation AUC: 0.5400\n",
      "Our out-of-fold mean training accuracy is 0.7625\n",
      "Our out-of-fold mean training AUC is 0.8352\n",
      "Our out-of-fold mean validation accuracy is 0.5800\n",
      "Our out-of-fold mean validation AUC is 0.5810\n",
      "Evaluating pipeline: AR_LogReg\n",
      "======= Fold 0 =======\n",
      "Training Accuracy: 0.9125, Training AUC: 0.9738\n",
      "Validation Accuracy: 0.7500, Validation AUC: 0.7700\n",
      "======= Fold 1 =======\n",
      "Training Accuracy: 0.9000, Training AUC: 0.9706\n",
      "Validation Accuracy: 0.8500, Validation AUC: 0.9200\n",
      "======= Fold 2 =======\n",
      "Training Accuracy: 0.9375, Training AUC: 0.9881\n",
      "Validation Accuracy: 0.6500, Validation AUC: 0.6700\n",
      "======= Fold 3 =======\n",
      "Training Accuracy: 0.8750, Training AUC: 0.9619\n",
      "Validation Accuracy: 0.8500, Validation AUC: 0.9400\n",
      "======= Fold 4 =======\n",
      "Training Accuracy: 0.9125, Training AUC: 0.9800\n",
      "Validation Accuracy: 0.6000, Validation AUC: 0.7400\n",
      "Our out-of-fold mean training accuracy is 0.9075\n",
      "Our out-of-fold mean training AUC is 0.9749\n",
      "Our out-of-fold mean validation accuracy is 0.7400\n",
      "Our out-of-fold mean validation AUC is 0.8080\n",
      "Evaluating pipeline: MeanAmp_NB\n",
      "======= Fold 0 =======\n",
      "Training Accuracy: 0.7375, Training AUC: 0.8650\n",
      "Validation Accuracy: 0.5000, Validation AUC: 0.6400\n",
      "======= Fold 1 =======\n",
      "Training Accuracy: 0.6750, Training AUC: 0.8087\n",
      "Validation Accuracy: 0.6500, Validation AUC: 0.7600\n",
      "======= Fold 2 =======\n",
      "Training Accuracy: 0.7250, Training AUC: 0.8094\n",
      "Validation Accuracy: 0.8000, Validation AUC: 0.7300\n",
      "======= Fold 3 =======\n",
      "Training Accuracy: 0.6625, Training AUC: 0.7650\n",
      "Validation Accuracy: 0.7500, Validation AUC: 0.8800\n",
      "======= Fold 4 =======\n",
      "Training Accuracy: 0.7875, Training AUC: 0.8450\n",
      "Validation Accuracy: 0.5500, Validation AUC: 0.6200\n",
      "Our out-of-fold mean training accuracy is 0.7175\n",
      "Our out-of-fold mean training AUC is 0.8186\n",
      "Our out-of-fold mean validation accuracy is 0.6500\n",
      "Our out-of-fold mean validation AUC is 0.7260\n",
      "Evaluating pipeline: ICA_PSD_SVM\n",
      "Fitting ICA to data using 1600 channels (please be patient, this may take a while)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py:102: RuntimeWarning: Channel names are not unique, found duplicates for: {'eeg'}. Applying running numbers for duplicates.\n",
      "  info = mne.create_info(ch_names=['eeg'] * X_concat.shape[0],\n",
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py:105: RuntimeWarning: The data has not been high-pass filtered. For good ICA performance, it should be high-pass filtered (e.g., with a 1.0 Hz lower bound) before fitting ICA.\n",
      "  self.ica.fit(raw)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.2s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py:112: RuntimeWarning: Channel names are not unique, found duplicates for: {'eeg'}. Applying running numbers for duplicates.\n",
      "  info = mne.create_info(ch_names=['eeg'] * n_channels,\n",
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py:102: RuntimeWarning: Channel names are not unique, found duplicates for: {'eeg'}. Applying running numbers for duplicates.\n",
      "  info = mne.create_info(ch_names=['eeg'] * X_concat.shape[0],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ICA to data using 1600 channels (please be patient, this may take a while)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py:105: RuntimeWarning: The data has not been high-pass filtered. For good ICA performance, it should be high-pass filtered (e.g., with a 1.0 Hz lower bound) before fitting ICA.\n",
      "  self.ica.fit(raw)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.6s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py:112: RuntimeWarning: Channel names are not unique, found duplicates for: {'eeg'}. Applying running numbers for duplicates.\n",
      "  info = mne.create_info(ch_names=['eeg'] * n_channels,\n",
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py:102: RuntimeWarning: Channel names are not unique, found duplicates for: {'eeg'}. Applying running numbers for duplicates.\n",
      "  info = mne.create_info(ch_names=['eeg'] * X_concat.shape[0],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ICA to data using 1600 channels (please be patient, this may take a while)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py:105: RuntimeWarning: The data has not been high-pass filtered. For good ICA performance, it should be high-pass filtered (e.g., with a 1.0 Hz lower bound) before fitting ICA.\n",
      "  self.ica.fit(raw)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.2s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py:112: RuntimeWarning: Channel names are not unique, found duplicates for: {'eeg'}. Applying running numbers for duplicates.\n",
      "  info = mne.create_info(ch_names=['eeg'] * n_channels,\n",
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py:102: RuntimeWarning: Channel names are not unique, found duplicates for: {'eeg'}. Applying running numbers for duplicates.\n",
      "  info = mne.create_info(ch_names=['eeg'] * X_concat.shape[0],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ICA to data using 1600 channels (please be patient, this may take a while)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py:105: RuntimeWarning: The data has not been high-pass filtered. For good ICA performance, it should be high-pass filtered (e.g., with a 1.0 Hz lower bound) before fitting ICA.\n",
      "  self.ica.fit(raw)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.1s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py:112: RuntimeWarning: Channel names are not unique, found duplicates for: {'eeg'}. Applying running numbers for duplicates.\n",
      "  info = mne.create_info(ch_names=['eeg'] * n_channels,\n",
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py:102: RuntimeWarning: Channel names are not unique, found duplicates for: {'eeg'}. Applying running numbers for duplicates.\n",
      "  info = mne.create_info(ch_names=['eeg'] * X_concat.shape[0],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ICA to data using 1600 channels (please be patient, this may take a while)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py:105: RuntimeWarning: The data has not been high-pass filtered. For good ICA performance, it should be high-pass filtered (e.g., with a 1.0 Hz lower bound) before fitting ICA.\n",
      "  self.ica.fit(raw)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting by number: 15 components\n",
      "Fitting ICA took 1.1s.\n",
      "An error occurred in pipeline ICA_PSD_SVM: \n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\pipeline.py\", line 469, in fit\n",
      "    Xt = self._fit(X, y, routed_params)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\pipeline.py\", line 406, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\pipeline.py\", line 1310, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\base.py\", line 1101, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py\", line 115, in transform\n",
      "    raw_ica = self.ica.apply(raw.copy(), exclude=[], verbose=False)\n",
      "  File \"<decorator-gen-413>\", line 10, in apply\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\mne\\preprocessing\\ica.py\", line 2237, in apply\n",
      "    out = meth(**kwargs)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\mne\\preprocessing\\ica.py\", line 2260, in _apply_raw\n",
      "    data = self._pick_sources(data, include, exclude, n_pca_components)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\mne\\preprocessing\\ica.py\", line 2318, in _pick_sources\n",
      "    data = self._pre_whiten(data)\n",
      "  File \"c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\mne\\preprocessing\\ica.py\", line 885, in _pre_whiten\n",
      "    data /= self.pre_whitener_\n",
      "ValueError: operands could not be broadcast together with shapes (20,2501) (1600,1) (20,2501) \n",
      "\n",
      "Evaluating pipeline: FFT_RF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\code\\pipelines\\ml_transformers.py:112: RuntimeWarning: Channel names are not unique, found duplicates for: {'eeg'}. Applying running numbers for duplicates.\n",
      "  info = mne.create_info(ch_names=['eeg'] * n_channels,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Fold 0 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.4000, Validation AUC: 0.4200\n",
      "======= Fold 1 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.6500, Validation AUC: 0.8050\n",
      "======= Fold 2 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.6500, Validation AUC: 0.5950\n",
      "======= Fold 3 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.7000, Validation AUC: 0.7100\n",
      "======= Fold 4 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.4000, Validation AUC: 0.5200\n",
      "Our out-of-fold mean training accuracy is 1.0000\n",
      "Our out-of-fold mean training AUC is 1.0000\n",
      "Our out-of-fold mean validation accuracy is 0.5600\n",
      "Our out-of-fold mean validation AUC is 0.6100\n",
      "Evaluating pipeline: TimeFreq_LDA\n",
      "======= Fold 0 =======\n",
      "Training Accuracy: 0.8125, Training AUC: 0.8919\n",
      "Validation Accuracy: 0.7000, Validation AUC: 0.7000\n",
      "======= Fold 1 =======\n",
      "Training Accuracy: 0.8250, Training AUC: 0.8894\n",
      "Validation Accuracy: 0.6000, Validation AUC: 0.5800\n",
      "======= Fold 2 =======\n",
      "Training Accuracy: 0.8125, Training AUC: 0.8544\n",
      "Validation Accuracy: 0.7500, Validation AUC: 0.7800\n",
      "======= Fold 3 =======\n",
      "Training Accuracy: 0.8000, Training AUC: 0.8650\n",
      "Validation Accuracy: 0.7000, Validation AUC: 0.8400\n",
      "======= Fold 4 =======\n",
      "Training Accuracy: 0.7750, Training AUC: 0.8944\n",
      "Validation Accuracy: 0.5500, Validation AUC: 0.5300\n",
      "Our out-of-fold mean training accuracy is 0.8050\n",
      "Our out-of-fold mean training AUC is 0.8790\n",
      "Our out-of-fold mean validation accuracy is 0.6600\n",
      "Our out-of-fold mean validation AUC is 0.6860\n",
      "Evaluating pipeline: PSD_XGB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:52:07] WARNING: D:\\bld\\xgboost-split_1727635034975\\work\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:52:09] WARNING: D:\\bld\\xgboost-split_1727635034975\\work\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:52:11] WARNING: D:\\bld\\xgboost-split_1727635034975\\work\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:52:14] WARNING: D:\\bld\\xgboost-split_1727635034975\\work\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\rokas\\Documents\\BCI\\mi-bci\\.pixi\\envs\\default\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:52:16] WARNING: D:\\bld\\xgboost-split_1727635034975\\work\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Fold 0 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.6500, Validation AUC: 0.7500\n",
      "======= Fold 1 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.9000, Validation AUC: 0.9900\n",
      "======= Fold 2 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.8500, Validation AUC: 0.9400\n",
      "======= Fold 3 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.9500, Validation AUC: 0.9800\n",
      "======= Fold 4 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.8500, Validation AUC: 0.9200\n",
      "Our out-of-fold mean training accuracy is 1.0000\n",
      "Our out-of-fold mean training AUC is 1.0000\n",
      "Our out-of-fold mean validation accuracy is 0.8400\n",
      "Our out-of-fold mean validation AUC is 0.9160\n",
      "Evaluating pipeline: Stacking\n",
      "======= Fold 0 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.5000, Validation AUC: 0.6200\n",
      "======= Fold 1 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.8000, Validation AUC: 0.9000\n",
      "======= Fold 2 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.6500, Validation AUC: 0.7500\n",
      "======= Fold 3 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.7000, Validation AUC: 0.7600\n",
      "======= Fold 4 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.6000, Validation AUC: 0.6900\n",
      "Our out-of-fold mean training accuracy is 1.0000\n",
      "Our out-of-fold mean training AUC is 1.0000\n",
      "Our out-of-fold mean validation accuracy is 0.6500\n",
      "Our out-of-fold mean validation AUC is 0.7440\n",
      "Evaluating pipeline: SRC\n",
      "An error occurred in pipeline SRC: Pipeline has none of the following attributes: decision_function, predict_proba.\n",
      "Evaluating pipeline: MLP_NN\n",
      "======= Fold 0 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.3500, Validation AUC: 0.2950\n",
      "======= Fold 1 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.3500, Validation AUC: 0.4000\n",
      "======= Fold 2 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.3500, Validation AUC: 0.3550\n",
      "======= Fold 3 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.6000, Validation AUC: 0.5400\n",
      "======= Fold 4 =======\n",
      "Training Accuracy: 1.0000, Training AUC: 1.0000\n",
      "Validation Accuracy: 0.4500, Validation AUC: 0.3950\n",
      "Our out-of-fold mean training accuracy is 1.0000\n",
      "Our out-of-fold mean training AUC is 1.0000\n",
      "Our out-of-fold mean validation accuracy is 0.4200\n",
      "Our out-of-fold mean validation AUC is 0.3970\n"
     ]
    }
   ],
   "source": [
    "epochs_train = epochs_for_train(epochs_p)\n",
    "results = train_and_evaluate(epochs_train, ml_pipelines, n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CSP_LogReg': {'train_accuracy': array([0.9875, 0.975 , 0.975 , 0.9875, 0.975 ]),\n",
       "  'train_roc_auc': array([0.995625, 0.993125, 0.99125 , 0.995   , 0.995   ]),\n",
       "  'val_accuracy': array([0.9 , 0.95, 0.95, 0.95, 0.9 ]),\n",
       "  'val_roc_auc': array([0.97, 0.99, 1.  , 0.99, 1.  ]),\n",
       "  'mean_train_accuracy': 0.9799999999999999,\n",
       "  'mean_train_auc': 0.994,\n",
       "  'mean_val_accuracy': 0.93,\n",
       "  'mean_val_auc': 0.99},\n",
       " 'PSD_SVM': {'train_accuracy': array([0.9375, 0.875 , 0.925 , 0.9   , 0.8875]),\n",
       "  'train_roc_auc': array([0.99    , 0.953125, 0.97125 , 0.971875, 0.981875]),\n",
       "  'val_accuracy': array([0.4 , 0.6 , 0.6 , 0.5 , 0.65]),\n",
       "  'val_roc_auc': array([0.47, 0.66, 0.6 , 0.68, 0.62]),\n",
       "  'mean_train_accuracy': 0.9049999999999999,\n",
       "  'mean_train_auc': 0.9736249999999999,\n",
       "  'mean_val_accuracy': 0.55,\n",
       "  'mean_val_auc': 0.6060000000000001},\n",
       " 'TimeDomain_RF': {'train_accuracy': array([1., 1., 1., 1., 1.]),\n",
       "  'train_roc_auc': array([1., 1., 1., 1., 1.]),\n",
       "  'val_accuracy': array([0.6 , 0.6 , 0.65, 0.6 , 0.5 ]),\n",
       "  'val_roc_auc': array([0.6 , 0.58, 0.64, 0.65, 0.51]),\n",
       "  'mean_train_accuracy': 1.0,\n",
       "  'mean_train_auc': 1.0,\n",
       "  'mean_val_accuracy': 0.5900000000000001,\n",
       "  'mean_val_auc': 0.5960000000000001},\n",
       " 'Hilbert_KNN': None,\n",
       " 'Wavelet_PSD_NB': None,\n",
       " 'CAR_CSP_DT': {'train_accuracy': array([ 1., nan,  1.,  1.,  1.]),\n",
       "  'train_roc_auc': array([ 1., nan,  1.,  1.,  1.]),\n",
       "  'val_accuracy': array([0.9 ,  nan, 1.  , 0.95, 0.95]),\n",
       "  'val_roc_auc': array([0.9 ,  nan, 1.  , 0.95, 0.95]),\n",
       "  'mean_train_accuracy': nan,\n",
       "  'mean_train_auc': nan,\n",
       "  'mean_val_accuracy': nan,\n",
       "  'mean_val_auc': nan},\n",
       " 'ICA_TimeDomain_SVM': None,\n",
       " 'CSP_LDA': {'train_accuracy': array([0.9875, 0.975 , 0.9375, 0.9875, 0.9625]),\n",
       "  'train_roc_auc': array([0.9975 , 0.99125, 0.99125, 0.99625, 0.99375]),\n",
       "  'val_accuracy': array([0.9 , 0.9 , 0.95, 0.95, 0.9 ]),\n",
       "  'val_roc_auc': array([0.97, 0.98, 0.99, 0.99, 1.  ]),\n",
       "  'mean_train_accuracy': 0.9700000000000001,\n",
       "  'mean_train_auc': 0.994,\n",
       "  'mean_val_accuracy': 0.9200000000000002,\n",
       "  'mean_val_auc': 0.9860000000000001},\n",
       " 'PSD_GB': {'train_accuracy': array([1., 1., 1., 1., 1.]),\n",
       "  'train_roc_auc': array([1., 1., 1., 1., 1.]),\n",
       "  'val_accuracy': array([0.7 , 0.85, 0.85, 0.8 , 0.7 ]),\n",
       "  'val_roc_auc': array([0.745, 0.96 , 0.835, 0.875, 0.71 ]),\n",
       "  'mean_train_accuracy': 1.0,\n",
       "  'mean_train_auc': 1.0,\n",
       "  'mean_val_accuracy': 0.78,\n",
       "  'mean_val_auc': 0.825},\n",
       " 'CAR_Riemann_LogReg': None,\n",
       " 'STFT_SVM': {'train_accuracy': array([1.    , 1.    , 0.9875, 1.    , 1.    ]),\n",
       "  'train_roc_auc': array([1., 1., 1., 1., 1.]),\n",
       "  'val_accuracy': array([0.7 , 0.7 , 0.55, 0.65, 0.45]),\n",
       "  'val_roc_auc': array([0.69, 0.72, 0.68, 0.77, 0.47]),\n",
       "  'mean_train_accuracy': 0.9974999999999999,\n",
       "  'mean_train_auc': 1.0,\n",
       "  'mean_val_accuracy': 0.6100000000000001,\n",
       "  'mean_val_auc': 0.666},\n",
       " 'Morlet_KNN': {'train_accuracy': array([0.7875, 0.75  , 0.8125, 0.825 , 0.6375]),\n",
       "  'train_roc_auc': array([0.8259375, 0.8634375, 0.8496875, 0.844375 , 0.7925   ]),\n",
       "  'val_accuracy': array([0.75, 0.55, 0.35, 0.65, 0.6 ]),\n",
       "  'val_roc_auc': array([0.685, 0.55 , 0.385, 0.745, 0.54 ]),\n",
       "  'mean_train_accuracy': 0.7625,\n",
       "  'mean_train_auc': 0.8351875,\n",
       "  'mean_val_accuracy': 0.58,\n",
       "  'mean_val_auc': 0.581},\n",
       " 'AR_LogReg': {'train_accuracy': array([0.9125, 0.9   , 0.9375, 0.875 , 0.9125]),\n",
       "  'train_roc_auc': array([0.97375 , 0.970625, 0.988125, 0.961875, 0.98    ]),\n",
       "  'val_accuracy': array([0.75, 0.85, 0.65, 0.85, 0.6 ]),\n",
       "  'val_roc_auc': array([0.77, 0.92, 0.67, 0.94, 0.74]),\n",
       "  'mean_train_accuracy': 0.9075,\n",
       "  'mean_train_auc': 0.9748750000000002,\n",
       "  'mean_val_accuracy': 0.74,\n",
       "  'mean_val_auc': 0.808},\n",
       " 'MeanAmp_NB': {'train_accuracy': array([0.7375, 0.675 , 0.725 , 0.6625, 0.7875]),\n",
       "  'train_roc_auc': array([0.865   , 0.80875 , 0.809375, 0.765   , 0.845   ]),\n",
       "  'val_accuracy': array([0.5 , 0.65, 0.8 , 0.75, 0.55]),\n",
       "  'val_roc_auc': array([0.64, 0.76, 0.73, 0.88, 0.62]),\n",
       "  'mean_train_accuracy': 0.7175,\n",
       "  'mean_train_auc': 0.8186249999999999,\n",
       "  'mean_val_accuracy': 0.65,\n",
       "  'mean_val_auc': 0.726},\n",
       " 'ICA_PSD_SVM': None,\n",
       " 'FFT_RF': {'train_accuracy': array([1., 1., 1., 1., 1.]),\n",
       "  'train_roc_auc': array([1., 1., 1., 1., 1.]),\n",
       "  'val_accuracy': array([0.4 , 0.65, 0.65, 0.7 , 0.4 ]),\n",
       "  'val_roc_auc': array([0.42 , 0.805, 0.595, 0.71 , 0.52 ]),\n",
       "  'mean_train_accuracy': 1.0,\n",
       "  'mean_train_auc': 1.0,\n",
       "  'mean_val_accuracy': 0.56,\n",
       "  'mean_val_auc': 0.6100000000000001},\n",
       " 'TimeFreq_LDA': {'train_accuracy': array([0.8125, 0.825 , 0.8125, 0.8   , 0.775 ]),\n",
       "  'train_roc_auc': array([0.891875, 0.889375, 0.854375, 0.865   , 0.894375]),\n",
       "  'val_accuracy': array([0.7 , 0.6 , 0.75, 0.7 , 0.55]),\n",
       "  'val_roc_auc': array([0.7 , 0.58, 0.78, 0.84, 0.53]),\n",
       "  'mean_train_accuracy': 0.805,\n",
       "  'mean_train_auc': 0.8790000000000001,\n",
       "  'mean_val_accuracy': 0.6599999999999999,\n",
       "  'mean_val_auc': 0.6859999999999998},\n",
       " 'PSD_XGB': {'train_accuracy': array([1., 1., 1., 1., 1.]),\n",
       "  'train_roc_auc': array([1., 1., 1., 1., 1.]),\n",
       "  'val_accuracy': array([0.65, 0.9 , 0.85, 0.95, 0.85]),\n",
       "  'val_roc_auc': array([0.75, 0.99, 0.94, 0.98, 0.92]),\n",
       "  'mean_train_accuracy': 1.0,\n",
       "  'mean_train_auc': 1.0,\n",
       "  'mean_val_accuracy': 0.8399999999999999,\n",
       "  'mean_val_auc': 0.916},\n",
       " 'Stacking': {'train_accuracy': array([1., 1., 1., 1., 1.]),\n",
       "  'train_roc_auc': array([1., 1., 1., 1., 1.]),\n",
       "  'val_accuracy': array([0.5 , 0.8 , 0.65, 0.7 , 0.6 ]),\n",
       "  'val_roc_auc': array([0.62, 0.9 , 0.75, 0.76, 0.69]),\n",
       "  'mean_train_accuracy': 1.0,\n",
       "  'mean_train_auc': 1.0,\n",
       "  'mean_val_accuracy': 0.6500000000000001,\n",
       "  'mean_val_auc': 0.744},\n",
       " 'SRC': None,\n",
       " 'MLP_NN': {'train_accuracy': array([1., 1., 1., 1., 1.]),\n",
       "  'train_roc_auc': array([1., 1., 1., 1., 1.]),\n",
       "  'val_accuracy': array([0.35, 0.35, 0.35, 0.6 , 0.45]),\n",
       "  'val_roc_auc': array([0.295, 0.4  , 0.355, 0.54 , 0.395]),\n",
       "  'mean_train_accuracy': 1.0,\n",
       "  'mean_train_auc': 1.0,\n",
       "  'mean_val_accuracy': 0.42000000000000004,\n",
       "  'mean_val_auc': 0.39699999999999996}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.base import clone\n",
    "import numpy as np\n",
    "\n",
    "def train_and_evaluate(X, y, pipelines, n_splits=5):\n",
    "    \"\"\"\n",
    "    Trains and evaluates multiple pipelines using a train-validation split and computes accuracy and AUC.\n",
    "\n",
    "    Parameters:\n",
    "    - X: EEG data array of shape (n_samples, n_channels, n_times)\n",
    "    - y: Labels array of shape (n_samples,)\n",
    "    - pipelines: Dictionary of sklearn Pipelines\n",
    "    - n_splits: Number of folds for cross-validation (not used in this version)\n",
    "\n",
    "    Returns:\n",
    "    - results: Dictionary containing accuracy and AUC scores for each pipeline\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        X, y, test_size=0.1, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    for name, pipeline in pipelines.items():\n",
    "        print(f\"Evaluating pipeline: {name}\")\n",
    "        try:\n",
    "            # Clone the pipeline to ensure a fresh model\n",
    "            clf = clone(pipeline)\n",
    "\n",
    "            # Fit the classifier on the training data\n",
    "            clf.fit(X_tr, y_tr)\n",
    "\n",
    "            # Predict on the validation set\n",
    "            pred = clf.predict(X_val)\n",
    "            if hasattr(clf, \"predict_proba\"):\n",
    "                pred_prob = clf.predict_proba(X_val)[:, 1]\n",
    "            else:\n",
    "                # Use decision function if predict_proba is not available\n",
    "                pred_prob = clf.decision_function(X_val)\n",
    "                # If decision_function returns shape (n_samples,), convert it to probabilities\n",
    "                pred_prob = (pred_prob - pred_prob.min()) / (pred_prob.max() - pred_prob.min())\n",
    "\n",
    "            # Compute accuracy and AUC scores\n",
    "            acc_score = accuracy_score(y_val, pred)\n",
    "            auc_score = roc_auc_score(y_val, pred_prob)\n",
    "\n",
    "            print(\n",
    "                f\"Our accuracy on the validation set is {acc_score:0.4f} and AUC is {auc_score:0.4f}\"\n",
    "            )\n",
    "\n",
    "            # Store the results\n",
    "            results[name] = {'accuracy': acc_score, 'auc': auc_score}\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred in pipeline {name}: {e}\")\n",
    "            results[name] = None\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CSP_LogReg': None,\n",
       " 'PSD_SVM': None,\n",
       " 'TimeDomain_RF': None,\n",
       " 'Hilbert_KNN': None,\n",
       " 'Wavelet_PSD_NB': None,\n",
       " 'CAR_CSP_DT': None,\n",
       " 'ICA_TimeDomain_SVM': None,\n",
       " 'CSP_LDA': None,\n",
       " 'PSD_GB': None,\n",
       " 'CAR_Riemann_LogReg': None,\n",
       " 'STFT_SVM': None,\n",
       " 'Morlet_KNN': None,\n",
       " 'AR_LogReg': None,\n",
       " 'MeanAmp_NB': None,\n",
       " 'ICA_PSD_SVM': None,\n",
       " 'FFT_RF': None,\n",
       " 'TimeFreq_LDA': None,\n",
       " 'PSD_XGB': None,\n",
       " 'Stacking': None,\n",
       " 'SRC': None,\n",
       " 'MLP_NN': None}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rokas\\AppData\\Local\\Temp\\ipykernel_27652\\2285067992.py:447: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  ('eegnet', KerasClassifier(build_fn=create_eegnet_model_wrapper, epochs=50, batch_size=16, verbose=0))\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import mne\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,\n",
    "                              VotingClassifier, StackingClassifier)\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "from mne.decoding import CSP, Vectorizer\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "\n",
    "# Import for Deep Learning models\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Conv2D, DepthwiseConv2D, SeparableConv2D,\n",
    "                                     BatchNormalization, Activation, AveragePooling2D,\n",
    "                                     Dropout, Flatten, Dense)\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# ===========================================\n",
    "# Custom Transformers (Use built-in where possible)\n",
    "# ===========================================\n",
    "\n",
    "# Since some transformations are not available in built-in libraries,\n",
    "# we define custom transformers only when necessary.\n",
    "\n",
    "class ReshapeTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Reshapes 3D EEG data to 2D for classifier input.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        return X.reshape(n_samples, -1)\n",
    "\n",
    "class PSDTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extracts Power Spectral Density (PSD) features.\"\"\"\n",
    "    def __init__(self, sfreq=256, fmin=0.1, fmax=40):\n",
    "        self.sfreq = sfreq\n",
    "        self.fmin = fmin\n",
    "        self.fmax = fmax\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        psd, freqs = mne.time_frequency.psd_array_multitaper(\n",
    "            X, sfreq=self.sfreq, fmin=self.fmin, fmax=self.fmax, verbose=False)\n",
    "        return psd.reshape(psd.shape[0], -1)\n",
    "\n",
    "class ARTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Computes Autoregressive (AR) coefficients.\"\"\"\n",
    "    def __init__(self, order=5):\n",
    "        self.order = order\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        from statsmodels.tsa.ar_model import AutoReg\n",
    "        n_samples, n_channels, n_times = X.shape\n",
    "        ar_features = np.zeros((n_samples, n_channels * self.order))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_channels):\n",
    "                model = AutoReg(X[i, j, :], lags=self.order, old_names=False)\n",
    "                model_fit = model.fit()\n",
    "                ar_coeffs = model_fit.params[1:]  # Exclude intercept\n",
    "                ar_features[i, j * self.order:(j + 1) * self.order] = ar_coeffs\n",
    "        return ar_features\n",
    "\n",
    "# ===========================================\n",
    "# Machine Learning Pipelines\n",
    "# ===========================================\n",
    "\n",
    "ml_pipelines = {}\n",
    "\n",
    "# 1. CSP + Logistic Regression\n",
    "ml_pipelines['CSP_LogReg'] = Pipeline([\n",
    "    ('csp', CSP(n_components=4, reg=None, log=True, norm_trace=False)),\n",
    "    ('logreg', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# 2. PSD + SVM\n",
    "ml_pipelines['PSD_SVM'] = Pipeline([\n",
    "    ('psd', PSDTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "# 3. Time Domain Features + Random Forest\n",
    "class TimeDomainTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Computes time-domain features like mean, variance, skewness.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        mean = np.mean(X, axis=2)\n",
    "        var = np.var(X, axis=2)\n",
    "        skewness = np.mean(((X - mean[:, :, np.newaxis]) ** 3), axis=2) / (var ** 1.5)\n",
    "        features = np.concatenate((mean, var, skewness), axis=1)\n",
    "        return features\n",
    "\n",
    "ml_pipelines['TimeDomain_RF'] = Pipeline([\n",
    "    ('time_features', TimeDomainTransformer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# 4. Hilbert Transform + k-NN\n",
    "class HilbertTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Applies the Hilbert transform to EEG data.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        analytic_signal = mne.filter.hilbert(X, picks=None, envelope=False, verbose=False)\n",
    "        amplitude_envelope = np.abs(analytic_signal)\n",
    "        return amplitude_envelope\n",
    "\n",
    "ml_pipelines['Hilbert_KNN'] = Pipeline([\n",
    "    ('hilbert', HilbertTransformer()),\n",
    "    ('reshape', ReshapeTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# 5. Wavelet Transform + PSD + Naive Bayes\n",
    "class WaveletTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Applies Wavelet Transform.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        import pywt\n",
    "        n_samples, n_channels, n_times = X.shape\n",
    "        wavelet_features = []\n",
    "        for i in range(n_samples):\n",
    "            sample_features = []\n",
    "            for j in range(n_channels):\n",
    "                coeffs = pywt.wavedec(X[i, j, :], 'db4', level=3)\n",
    "                coeffs_flat = np.concatenate([c.flatten() for c in coeffs])\n",
    "                sample_features.append(coeffs_flat)\n",
    "            wavelet_features.append(np.concatenate(sample_features))\n",
    "        return np.array(wavelet_features)\n",
    "\n",
    "ml_pipelines['Wavelet_PSD_NB'] = Pipeline([\n",
    "    ('wavelet', WaveletTransformer()),\n",
    "    ('psd', PSDTransformer()),\n",
    "    ('nb', GaussianNB())\n",
    "])\n",
    "\n",
    "# 6. CAR + CSP + Decision Tree\n",
    "class CARTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Applies Common Average Referencing (CAR).\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        car = X - np.mean(X, axis=1, keepdims=True)\n",
    "        return car\n",
    "\n",
    "ml_pipelines['CAR_CSP_DT'] = Pipeline([\n",
    "    ('car', CARTransformer()),\n",
    "    ('csp', CSP(n_components=4, reg=None, log=True, norm_trace=False)),\n",
    "    ('dt', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "# 7. ICA + Time Domain Features + SVM\n",
    "class ICATransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Applies Independent Component Analysis (ICA).\"\"\"\n",
    "    def __init__(self, n_components=15, random_state=42):\n",
    "        self.n_components = n_components\n",
    "        self.random_state = random_state\n",
    "        self.ica = None\n",
    "    def fit(self, X, y=None):\n",
    "        n_samples, n_channels, n_times = X.shape\n",
    "        X_concat = X.reshape(n_samples * n_channels, n_times)\n",
    "        self.ica = mne.preprocessing.ICA(n_components=self.n_components,\n",
    "                                         random_state=self.random_state,\n",
    "                                         max_iter='auto', verbose=False)\n",
    "        info = mne.create_info(ch_names=['eeg'] * X_concat.shape[0],\n",
    "                               sfreq=256, ch_types='eeg')\n",
    "        raw = mne.io.RawArray(X_concat, info, verbose=False)\n",
    "        self.ica.fit(raw)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        n_samples, n_channels, n_times = X.shape\n",
    "        X_transformed = []\n",
    "        for i in range(n_samples):\n",
    "            data = X[i]\n",
    "            info = mne.create_info(ch_names=['eeg'] * n_channels,\n",
    "                                   sfreq=256, ch_types='eeg')\n",
    "            raw = mne.io.RawArray(data, info, verbose=False)\n",
    "            raw_ica = self.ica.apply(raw.copy(), exclude=[], verbose=False)\n",
    "            X_transformed.append(raw_ica.get_data())\n",
    "        return np.array(X_transformed)\n",
    "\n",
    "ml_pipelines['ICA_TimeDomain_SVM'] = Pipeline([\n",
    "    ('ica', ICATransformer(n_components=15)),\n",
    "    ('time_features', TimeDomainTransformer()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "# 8. CSP + LDA\n",
    "ml_pipelines['CSP_LDA'] = Pipeline([\n",
    "    ('csp', CSP(n_components=4, reg=None, log=True, norm_trace=False)),\n",
    "    ('lda', LDA())\n",
    "])\n",
    "\n",
    "# 9. PSD + Gradient Boosting\n",
    "ml_pipelines['PSD_GB'] = Pipeline([\n",
    "    ('psd', PSDTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "# 10. CAR + Riemannian Geometry Features + Logistic Regression\n",
    "ml_pipelines['CAR_Riemann_LogReg'] = Pipeline([\n",
    "    ('car', CARTransformer()),\n",
    "    ('cov', Covariances()),\n",
    "    ('ts', TangentSpace()),\n",
    "    ('logreg', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# 11. STFT + SVM\n",
    "class STFTTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Computes Short-Time Fourier Transform (STFT).\"\"\"\n",
    "    def __init__(self, n_fft=256):\n",
    "        self.n_fft = n_fft\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        from scipy.signal import stft\n",
    "        n_samples, n_channels, n_times = X.shape\n",
    "        stft_features = []\n",
    "        for i in range(n_samples):\n",
    "            sample_features = []\n",
    "            for j in range(n_channels):\n",
    "                _, _, Zxx = stft(X[i, j, :], nperseg=self.n_fft)\n",
    "                sample_features.append(np.abs(Zxx).flatten())\n",
    "            stft_features.append(np.concatenate(sample_features))\n",
    "        return np.array(stft_features)\n",
    "\n",
    "ml_pipelines['STFT_SVM'] = Pipeline([\n",
    "    ('stft', STFTTransformer(n_fft=256)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "# 12. Morlet Wavelet Transform + k-NN\n",
    "class MorletWaveletTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Applies time-frequency analysis using Morlet wavelets.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        freqs = np.linspace(8, 30, num=22)\n",
    "        n_samples, n_channels, n_times = X.shape\n",
    "        power_features = []\n",
    "        for i in range(n_samples):\n",
    "            sample_features = []\n",
    "            for j in range(n_channels):\n",
    "                power = mne.time_frequency.tfr_array_morlet(\n",
    "                    X[i:i+1, j:j+1, :], sfreq=256, freqs=freqs,\n",
    "                    n_cycles=7, output='power', verbose=False)\n",
    "                sample_features.append(power.flatten())\n",
    "            power_features.append(np.concatenate(sample_features))\n",
    "        return np.array(power_features)\n",
    "\n",
    "ml_pipelines['Morlet_KNN'] = Pipeline([\n",
    "    ('morlet', MorletWaveletTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# 13. AR Coefficients + Logistic Regression\n",
    "ml_pipelines['AR_LogReg'] = Pipeline([\n",
    "    ('ar', ARTransformer(order=5)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# 14. Mean Amplitude Features + Naive Bayes\n",
    "class MeanAmplitudeTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Computes the mean amplitude of the signal.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        mean_amplitude = np.mean(np.abs(X), axis=2)\n",
    "        return mean_amplitude\n",
    "\n",
    "ml_pipelines['MeanAmp_NB'] = Pipeline([\n",
    "    ('mean_amp', MeanAmplitudeTransformer()),\n",
    "    ('nb', GaussianNB())\n",
    "])\n",
    "\n",
    "# 15. ICA + PSD + SVM\n",
    "ml_pipelines['ICA_PSD_SVM'] = Pipeline([\n",
    "    ('ica', ICATransformer(n_components=15)),\n",
    "    ('psd', PSDTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "# 16. FFT + Random Forest\n",
    "class FFTTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Computes Fast Fourier Transform (FFT).\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        fft_coeffs = np.fft.rfft(X, axis=2)\n",
    "        fft_features = np.abs(fft_coeffs)\n",
    "        return fft_features.reshape(X.shape[0], -1)\n",
    "\n",
    "ml_pipelines['FFT_RF'] = Pipeline([\n",
    "    ('fft', FFTTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# 17. Time-Frequency Features (STFT) + LDA\n",
    "ml_pipelines['TimeFreq_LDA'] = Pipeline([\n",
    "    ('stft', STFTTransformer(n_fft=256)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lda', LDA())\n",
    "])\n",
    "\n",
    "# 18. PSD Features + XGBoost Classifier\n",
    "ml_pipelines['PSD_XGB'] = Pipeline([\n",
    "    ('psd', PSDTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('xgb', xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n",
    "])\n",
    "\n",
    "# 19. Ensemble Methods (Stacking Classifier)\n",
    "ml_pipelines['Stacking'] = Pipeline([\n",
    "    ('psd', PSDTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('stacking', StackingClassifier(\n",
    "        estimators=[\n",
    "            ('svm', SVC(probability=True)),\n",
    "            ('rf', RandomForestClassifier()),\n",
    "            ('knn', KNeighborsClassifier())\n",
    "        ],\n",
    "        final_estimator=LogisticRegression(max_iter=1000),\n",
    "        cv=5\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 20. Sparse Representation Classification (SRC)\n",
    "class SRCClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Simulates a Sparse Representation Classifier using Lasso.\"\"\"\n",
    "    def __init__(self, alpha=0.1):\n",
    "        self.alpha = alpha\n",
    "        self.classes_ = None\n",
    "        self.dictionary_ = None\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.dictionary_ = {}\n",
    "        for cls in self.classes_:\n",
    "            self.dictionary_[cls] = X[y == cls].T\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        preds = []\n",
    "        for x in X:\n",
    "            residuals = []\n",
    "            for cls in self.classes_:\n",
    "                lasso = Lasso(alpha=self.alpha, max_iter=1000)\n",
    "                lasso.fit(self.dictionary_[cls], x)\n",
    "                reconstruction = lasso.predict(self.dictionary_[cls])\n",
    "                residual = np.linalg.norm(x - reconstruction)\n",
    "                residuals.append(residual)\n",
    "            preds.append(self.classes_[np.argmin(residuals)])\n",
    "        return np.array(preds)\n",
    "\n",
    "ml_pipelines['SRC'] = Pipeline([\n",
    "    ('reshape', ReshapeTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('src', SRCClassifier(alpha=0.1))\n",
    "])\n",
    "\n",
    "# 21. Multilayer Perceptron (MLP) Neural Network (As ML Pipeline)\n",
    "ml_pipelines['MLP_NN'] = Pipeline([\n",
    "    ('reshape', ReshapeTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('mlp', MLPClassifier(hidden_layer_sizes=(100,), max_iter=500))\n",
    "])\n",
    "\n",
    "# ===========================================\n",
    "# Neural Network Pipelines\n",
    "# ===========================================\n",
    "\n",
    "nn_pipelines = {}\n",
    "\n",
    "# 1. EEGNet (Deep CNN)\n",
    "class EEGNetTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Transformer that reshapes data for EEGNet.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        # Reshape X to (n_samples, n_channels, n_times, 1)\n",
    "        n_samples, n_channels, n_times = X.shape\n",
    "        X_reshaped = X.reshape(n_samples, n_channels, n_times, 1)\n",
    "        return X_reshaped\n",
    "\n",
    "def create_eegnet_model(n_channels, n_times, n_classes):\n",
    "    \"\"\"Creates an EEGNet model.\"\"\"\n",
    "    input_shape = (n_channels, n_times, 1)\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Block 1\n",
    "    x = Conv2D(16, (1, 64), padding='same',\n",
    "               use_bias=False)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = DepthwiseConv2D((n_channels, 1), use_bias=False,\n",
    "                        depth_multiplier=2,\n",
    "                        depthwise_constraint=max_norm(1.))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 4))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = SeparableConv2D(32, (1, 16), use_bias=False, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = AveragePooling2D((1, 8))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dense(n_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Placeholder function to create the model with the correct input dimensions\n",
    "def create_eegnet_model_wrapper():\n",
    "    n_channels = X.shape[1]\n",
    "    n_times = X.shape[2]\n",
    "    n_classes = len(np.unique(y))\n",
    "    return create_eegnet_model(n_channels, n_times, n_classes)\n",
    "\n",
    "nn_pipelines['EEGNet_CNN'] = Pipeline([\n",
    "    ('reshape', EEGNetTransformer()),\n",
    "    ('eegnet', KerasClassifier(build_fn=create_eegnet_model_wrapper, epochs=50, batch_size=16, verbose=0))\n",
    "])\n",
    "\n",
    "# ===========================================\n",
    "# Training Function with K-Fold Cross-Validation\n",
    "# ===========================================\n",
    "\n",
    "def train_and_evaluate(X, y, pipelines, n_splits=5):\n",
    "    \"\"\"\n",
    "    Trains and evaluates multiple pipelines using k-fold cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - X: EEG data array of shape (n_samples, n_channels, n_times)\n",
    "    - y: Labels array of shape (n_samples,)\n",
    "    - pipelines: Dictionary of sklearn Pipelines\n",
    "    - n_splits: Number of folds for cross-validation\n",
    "\n",
    "    Returns:\n",
    "    - results: Dictionary containing cross-validation scores for each pipeline\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for name, pipeline in pipelines.items():\n",
    "        print(f\"Evaluating pipeline: {name}\")\n",
    "        try:\n",
    "            if name == 'EEGNet_CNN':\n",
    "                # Use StratifiedKFold for EEGNet to maintain class balance\n",
    "                skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "                scores = []\n",
    "                for train_index, test_index in skf.split(X, y):\n",
    "                    X_train, X_test = X[train_index], X[test_index]\n",
    "                    y_train, y_test = y[train_index], y[test_index]\n",
    "                    pipeline.fit(X_train, y_train)\n",
    "                    score = pipeline.score(X_test, y_test)\n",
    "                    scores.append(score)\n",
    "                scores = np.array(scores)\n",
    "            else:\n",
    "                kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "                scores = cross_val_score(pipeline, X, y, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "            print(f\"Scores: {scores}\")\n",
    "            print(f\"Mean accuracy: {np.mean(scores):.4f}\")\n",
    "            results[name] = scores\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred in pipeline {name}: {e}\")\n",
    "            results[name] = None\n",
    "    return results\n",
    "\n",
    "# ===========================================\n",
    "# Example Usage\n",
    "# ===========================================\n",
    "\n",
    "# Assuming `X` and `y` are your EEG data and labels\n",
    "# Adjust the code to match your data loading and preprocessing steps\n",
    "\n",
    "# Example call for Machine Learning Pipelines\n",
    "# results_ml = train_and_evaluate(X, y, ml_pipelines, n_splits=5)\n",
    "\n",
    "# Example call for Neural Network Pipelines\n",
    "# results_nn = train_and_evaluate(X, y, nn_pipelines, n_splits=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
